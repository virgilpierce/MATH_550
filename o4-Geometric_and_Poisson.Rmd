# Geometric and Poisson Distributions

We can use the Bernouli Trials we met in the last chapter to build two other distributions that describe common situations (and also have enough symmetry they are computable).

## Geometric Distributions

This question worked better in Texas than it does in Colorado because of mail in voting, but still:  We are trying to quickly estimate what proportion of voters in Greeley are casting their ballots for candidate A running for mayor of the city. We do this by standing outside a voting location and asking candidates whether they voted for A or B and counting how many people we have to ask before we find our first person voting for A. Say for example that the 4th person we ask says they are voting for candidate A.

Let's see if we can use that number to estimate what proportion of the population p is voting for A. 

Assuming that our sample is random (and note that's the problem with Colorado voting) we could think of each time we ask a voter leaving the polling location who they voted for as a Bernouli trial with a probability of success p (granted we don't know p, but that's why we study math!).

So the question becomes:  If Y is the number of times we had to conduct our Bernouli trial before we got our first success, what is $P(Y=4)$?  Note that in order for trial 4 to be our first success 4 things had to happen - we had to have 3 trials in succession that were failures, each with a probability of 1-p; and then we had to have one trial that was a success with probability p. Provided that these events are independent then the likliehood of all 4 of them happening is just the product of these 4 factors:

$$ (1-p)^3 p $$

More generally the *geometric distribution* has a probability distribution function given by:

$$ P(Y=r+1) = (1-p)^{r} p $$

This is fundamentally different from our Binomial Distribution. For the binomial distribution with $n$ trials the largest the random variable could be is $n$. In this case, for our geometric distribution $Y$ is potentially unbounded.  For every $r$ the value of $P(Y=r+1)$ is non-zero (though possibly very small). 

It's helpful to fix a $p$ and plot the probabilities of $Y=r+1$ for the first few r.  The thing to notice in the plot is that in order for the random variable to have a chance of getting large, p needs to be small so that there is a large probability of the failures in the trial stacking up. Depending on the value of p you put in, you may need to extend the values r is using by adjusting the 25.

```{r}
p <- 0.51
r <- c(0:25)
v <- (1-p)^(r) * p
plot(r, v)
```

So for example:  If candidate A has 0.51 of the population supporting them, the probability that we needed to ask 4 voters before we found the first one voting for candidate A is:

```{r}
(1-0.51)^3 * 0.51
```

Meaning if we did this experiment at 100 polling sites or 100 different times, 6 of those times we would expect to wait until the 4th voter. But wait there is more. 

What we really care about is the complement of the cummulative distribution:  What is the probability that we had to wait until the 4th or **LATER** voter to find the first vote for A. This is a better way to ask because we are really asking how likely is it that candidate A with 0.51 of the vote, is doing this badly OR worse in the voters we have talked to. Note that the complement of this event is:  What is the probability that we found the first voter for A in 0, 1, 2, or 3 voters questioned?  We can than subtract that number from 1 to answer the question above.

```{r}
c <- (1-0.51)^0 * 0.51 + (1-0.51)^1 * 0.51 + (1-0.51)^2 * 0.51
c
```

R has a cumulative distribution for the geometric distribution coded in *pgeom* (note that there is an annoying thing in R where its definition of the geometric distribution is off by 1 from others):

```{r}
pgeom(2, 0.51)
```

and then subtracking this from 1 we get:

```{r}
1 - pgeom(2, 0.51)
```

Meaning that even for a candidate with 0.51 of the vote, we are not overly surprised when we have to talk to 4 voters before we find a supporter. 

It is interesting to ask, how does this probability change as p changes:

```{r}
p <- c(0:1000)/1000
v <- 1 - pgeom(2, p)
plot(p, v, type="l")
```


### Expected Value

The expected value can be computed exactly, and in this case we sort of prefer to do it that way as note that the one thing computers are not great at is infinite sums and infinite integrals. However the values of the distribution decay exponentially and so we actually do expect sums over the distribution to converge quickly. So we can find the approximate expected value by computing 

$$ \sum_{r=0}^N (r+1) p(r) $$ 

for a medium to large value of $N$. 

```{r}
N <- 100
r <- c(0:N)
p <- 0.51
sum( (r+1)*(1-p)^r * p )
```

Can you make a prediction about the exact value?

#### Exact Value

To compute the exact value of the sum, we just do it, using our one trick for adding up infinite sums - *write it in terms of a geometric series*.

$$ E(Y) = \sum_{r=0}^\infty (r+1) q^r p = p \frac{d}{dq} \sum_{r=0}^\infty q^{r+1} $$

The sum is now a geometric series and we have a formula for its result:

$$ E(Y) = p \frac{d}{dq} \left[ \frac{q}{1-q} ] = \mbox{algebra/calculus or calcugebra} = \frac{1}{p} $$

### Variance 

The approximate variance can be found using the formula $\sigma^2 = V(Y) = E(Y^2) - \mu^2$ with $\mu = E(Y)$.  For example:

```{r}
N <- 100
r <- c(0:N)
p <- 0.51
sum( (r+1)^2 * (1-p)^r * p )  - 1/p^2
```

#### Exact Value

To compute the exact value of the sum:

$$ \sigma^2 = V(Y) = E(Y^2) - \frac{1}{p^2} = E(Y (Y+1) ) - \frac{1}{p} - \frac{1}{p^2} $$

So we look for:

$$ E(Y (Y+1) ) = \sum_{r=0}^\infty (r+1) (r+2) q^r p = p \frac{d^2}{dq^2} \sum_{r=0}^\infty q^{r+2} = p \frac{d^2}{dq^2} \frac{q^2}{1-q} $$

Some Calculus and Algebra later:

$$ E(Y(Y+1)) = \frac{2}{p^2} $$

Putting it all together:

$$ V(Y) = \frac{2}{p^2} - \frac{1}{p} - \frac{1}{p^2} = \frac{1-p}{p^2} $$

### Poisson Distributions









