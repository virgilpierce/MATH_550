[["index.html", "MATH 550 - Applied Probability and Statistics 1 Introduction and Course Syllabus 1.1 Class and Instructor Details 1.2 No Textbook Purchase Required 1.3 Course Description and Prerequisites 1.4 Tentative Outline 1.5 Programing 1.6 Course Pre-requisites 1.7 Course Goals 1.8 Course Structure 1.9 Our compact with each other 1.10 Assignments 1.11 Grades 1.12 UNCO Policy Statements", " MATH 550 - Applied Probability and Statistics Virgil U Pierce 2021-05-24 1 Introduction and Course Syllabus Welcome to MATH 550 - Applied Probability and Statistics. 1.1 Class and Instructor Details MATH 550-901 (3 credits) Virgil U. Pierce Email: virgil.pierce@unco.edu Cell: 956-249-0566 Office: Ross Hall 2239 Class Meetings: 6/14 - 6/17: MTWR 1:00p - 4:00p 6/21 - 7/25: MW 1:00p - 4:00p 1.2 No Textbook Purchase Required Extensive notes for class will be distributed along with links to websites and other sources of information for the class. 1.3 Course Description and Prerequisites From the catalog: Methods related to descriptive and inferential statistics and the concept of probability are investigated in depth. Questions you will explore this summer: What is statistics and what are data? What is randomness and how do we describe long-run behavior? How do we describe our data? How do we make decisions from data? How do we use more data to update our decisions? Can we use these ideas to do computations in other areas of mathematics? 1.4 Tentative Outline Chapter 2: Introduction and using R Chapter 3: Discrete Probability and Binomial Distribution Chapter 4: Geometric and Poisson Distribution Chapter 5: Continuous Probability Chapter 6: Sampling Chapter 7: Hypothesis Testing and Confidence Intervals Chapter 8: Bayes Probability and Updating from New Data Chapter 9: Correlations and models. 1.5 Programing Statistics is best experienced through doing the work rather than concentrating on theory and as such we need to use software for the course to do the necessary computations. This course will serve as an introduction to the statistics software R, which is widely used for statistical computations. It is freely available, open source, and has an extensive collection of help online for it. If you are new to programming, R is a great place to learn because most of the tasks we do will involve just a few lines of commands. No prior knowledge of programming is assumed. The course materials are being written in R and are available in Github for you at this link: https://github.com/virgilpierce/MATH_550 From there you can download individual files from the class or the entire course as a Zip file. Updates will be pushed to that site as they are prepared. To access R and the editing software I recommend for using R and RStudio, you have options: You can download Rstudio including R for your Windows or Mac computer by going here: https://www.rstudio.com/products/rstudio/download/ Be sure to choose the free option - you will not need the paid versions. If you do not want to or cannot install Rstudio on your computer (for example if you are using a Tablet or Chromebook) you can use cocalc to run R commands: https://cocalc.com You can log in to the Apporto Virtual Server https://unco.apporto.com/ with your UNC credentials and run Rstudio there. 1.6 Course Pre-requisites Students are expected to have experience with Calculus 1 and 2, however we will use Wolfram Alpha for any and all calculus computations so it is more the concepts of Derivative - as slope or rate of change; Integral - as area or volume; and the Fundamental Theorem of Calculus that connects these two ideas. 1.7 Course Goals Students will develop ways of thinking about data that will guide them as critical consumers of statistics in 2021 and beyond. Students will develop coherent meanings for specific statistical concepts (and mathematical concepts) so that the use of the statistics becomes useful in their academic, professional, and personal lives. Students will improve their problem-solving abilities. Students will make logical and verbal arguments in support of their conclusions. 1.8 Course Structure Class will be a mixture of class discussion and group work. If you have been in my classes before it will be a familiar format. We will use Google Docs to collaborate during class, but you may be asked to share your screen to show the computation in R that you are doing. You will need to be current on the homework for class in order to participate effectively in class. Collected work will be based on the discussion from class. 1.9 Our compact with each other Our agreement is: Speak with Meaning - reference the attributes and using names for quantities and qualities (and people). Exhibit Intellectual Integrity - base your conjecture on a logical foundation; do not pretend to an understanding. We are all still learning, and admitting that is an important step forward. Strive to Make Sense - persist in making sense of your peer’s thinking. Almost every problem we do in class has a variety of approaches and a variety of answers. Respect the Learning Process of All - allow others time to think, reflect, and construct. Pose questions of others. 1.10 Assignments Projects: There will be two take home projects and a final project. You will have at least five days to complete these. They will be partly computational, and partly reflective. They will also have components that are unique to each individual. They are open notes, and in particular any resource online about using R is available provided it is not in the format of a Q&amp;A. If at all in doubt about a resource you have found, please ask. All resources used should be cited. Homework: The homework is posed with the intent that you will collaborate with others on it, and you are encouraged to do so. Your instructor is available during the week to consult with as well. Questions: You will occasionally be asked to compose a question for others in the class. These assignments will be collected during class. You will provide your question with a description of how it should be solved (from your point of view) and how it addresses the goal or concept. Evidence of Learning: Your grade in the class is going to be based on your progress towards answering the questions posed in the Course Description. Evidence will naturally come from your submissions for the three types of assignments above, however you are encouraged to consider other evidence that could be provided. 1.11 Grades Your grade will be based upon the 4 assignment types above and the extent to which they contain evidence that the course goals have been met. Exceeding expectations on all four course goals will result in a grade of A; a grade of B will mean that all of the course goals were met and on at least one the expectation was exceeded; a grade of C will mean that all of the course goals were met; a grade of D will mean that some of the course goals were met but not all of them; and a grade of F will mean that none of the course goals were met. 1.12 UNCO Policy Statements 1.12.1 Disability Resources It is the policy and practice of the University of Nothern Colorado to create inclusive learning environments - including this class. If there are aspects of the instruction or design of this course that present barriers to your inclusion or to an accurate assessment of your achievement [learning] such as time-limited exams, inaccessible web content, or use of videos without captions, please communicate this with your professor and contact Disability Support Services (DSS) at (970) 351-2289, Michener Library L-80 to request accommodations. Students can learn more about the accommodation process at https://www.unco.edu/disability-resource-center/accomodations/ 1.12.2 Food Insecurity and Basic Needs Research shows that college students experience food insecurity at higher rates than the American household rate, and that food insecurity can negatively impact academic performance and persistence. In recognition of this problem, UNC offers assistance to students facing food insecurity through an on-campus food pantry. The Bear Pantry is located in University Center 2166A, and is open for regular hours throughout the semester. Please visit http://www.unco.edu/bear-pantry for more information. Any student who faces challenges securing their food or housing and believes this may affect their performance in the course is also urged to contact Student Outreach and Support (SOS) for assistance. SOS can assist students during difficult circumstances which may include medical, mental health, personal or family crisis, illness or injury. SOS can be reached at sos@unco.edu or via phone at 970-351-2796. 1.12.3 Honor Code All members of the Univeristy of Northern Colorado community are entrustred with the responsibility to uphold and promote five fundamental values: Honesty, Trust, Respect, Fairness, and Responsibility. These core elements foster an atmosphere, insite and outside the classroom, which serves as a foundation and guides the UNC community’s academic, professional, and personal growth. Endorsement of these core elements by students, faculty, staff, administration, and trustees strengthens the integrity and value of our academic climate. 1.12.4 UNC’s Policies There are more university policies not printed here, but published in the course catalog, university regulations, and the UNC Board of Trustees Policy Manual. In particular we will follow UNC’s polices and recomendations for academic misconduct. For additional information, please see the Student Code of Conduct at the Dean of Student’s website: http://wwww.unco.edu/dos/Conduct/codeofconduct.html. In the case of academic appeals, university procedures will be followed. For information on academic appeals see: https://www.unco.edu/trustees/pdf/board-policy-manual.pdf. "],["what-is-statistics.html", "2 What is Statistics? 2.1 Introduction and R 2.2 Plots 2.3 Categorical Data Example 2.4 Exploratory versus Confirmatory Analysis 2.5 Correlations and Predictions 2.6 Motivating Questions", " 2 What is Statistics? Our first unit will cover: How and why we will be using R in this course. A discussion of What is Statistics? A discussion of the differences between Exploratory versus Confirmatory analysis An overview of some of the problems we will develop tools to answer in this course 2.1 Introduction and R This course is going to use R for the computations. Much of what we do could be computed using Excell or Google Sheets however R has certain advantages over these tools: R is a purpose built statistics software. It is designed to do that really well. R arguably is the best tool for producing good looking figures suitable for publications. The figures are easy to manipulate and adjust. Most importantly it is really easy to have your figures be consistently formatted, something that is much harder with spreadsheets. R includes a markdown language for creating nice looking notebooks: It can be used as an all in one publication tool to produce a report or paper with statistical analysis. The markdown language includes LaTex for typsetting mathematics: \\[ \\int_a^b f(x) dx \\] R is an industry standard tool for doing statistics. and most importantly R is open and free. You can download it, and the tools we will use to interact with it Rstudio for free. It is also possible to use R in online environments like Cocalc. If you are teaching a statistics class for your high school or college, I highly recommend using R as part of your class. 2.1.1 What do you need To get started: If you are using a Mac or PC you should download a copy of RStudio from https://www.rstudio.com/products/rstudio/ the free version is all you will need. This includes a copy of R itself - Rstudio is a tool for interacting with R to make some of our tasks easier. If you are using a Chromebook, want to use R on a tablet, or just don’t want to install it I have two options for you: Cocalc provides an online interface that lets you issue R commands. It also includes a markdown language so you will be able to use it to prepare the documents for class. You can reach it here: https://cocalc.com/ you will need to create an account. The university Apporto virtual machine: https://unco.apporto.com has R studio installed. You can log in to it from any browser. If you are in Greeley you can use R on the machines in the computer labs on campus. Near my office is Ross 2261 and 2263. Note that parking is enforced in the summer, and that the building may be locked on the weekend. If at all possible I would recommend 1. as with the software installed locally you will not need to be using your internet connection. However students using 2. in my classes have reported that it works well for them. 2.1.2 Open Rstudio To actually start using R, go ahead and open R studio. You can use R directly in the console (lower-left quadrant). Or you can open a notebook. Notebooks allow you to include markdown explanations around blocks of R code. I will show you an example in class. I typically use a notebook for my work, but the console is easier particularly if you are in a exploratory phase. The frame in the upper-right contains an overview of the variables that have been assigned values; and then the frame in the lower-right gives you an view of the files in your working directory, and if you issue plot commands in the console they will display here. The button on the top-left of the tool bar is for opening new documents, click the triangle next to it and choose “New Notebook” form the menu. RStudio helpfully inserts some information setting up your new notebook. You can adjust it as you wish. You can save the document from the file menu or using the toolbar buttons. RStudio has provided an example of how to put in Markdown language, and how to put in R commands. You execute a section of R commands but putting the cursor inside of it and pressing “Control+Enter” (or on a mac “Command+Enter”, if using Cocalc you will need to press “Shift+Enter”). Try it now on the plot(cars) command. Note that from our textbook you can copy commands to your computers clipboard which you can then paste and run in the console or paste into a notebook to run - you will have to inclose them in the code enviroment to run. plot(cars) 2.1.3 A few words about programming languages Before we continue with the Probability and Statistics I want to just add that learning programming languages is a powerful tool to help your students advance in their thinking about mathematics, one that I worry we do not spend enough time working with students on. At UNC one of my main goals for my tenure as School Director is to incorporate Python and R into the undergraduate mathematics curriculum wherever we can. A key part of learning a programming language, or more accurately, the only thing that has ever worked for me is having a problem I am interested in that needs solving. Statistics for myself, I hope for you by the end of this course, and for your students could be that problem that motivates learning some programming. Independent of all of that R is a great language to start with for the following reasons: Because R is so focused on statistics and visualizations of data, it is simpler and more compact than a general language. R is human readable - you can usually read a statement outloud and you will know exactly what it is doing. R is free. If you are interested in more about how to introduce students to R in a statistics class, or Python in an algebra, trigonometry, or calculus class, please let me know. It is a topic I am very interested in. 2.1.4 A first example How many students were in each of your classes this Spring? While I am collecting this data you can reflect on this funny musing. I grew up in rural Vermont and one thing I remember learning as a child was that asking a farmer how many cows they had was very rude, equivalent to asking how much money they had. I’ve always felt a bit strange asking other teachers how many students they have because of this. # for testing use this students &lt;- round(rnorm(20, 25, 5)) # for class use this # students &lt;- c() students ## [1] 35 21 24 23 38 27 20 31 24 30 30 24 20 32 23 28 21 21 30 21 R Notes: c() stands for column and is one of the most basic data structures in R. Most things we will build will either manipulate columns directly or will use structures built out of columns. Comments in commands start with hash symbols. Another syntax note: R uses both = and &lt;- for assignment though they have slightly different meanings. They both work as you would expect, however &lt;- has some nice positives in the detailed differences and it is worth getting into the habit of using it. RStudio Note: You might notice that the variable we have just created has appeared in the upper-right quadrant of RStudio. 2.1.4.1 Summary Statistics To get started we can use summary to get an overview of what our list of student enrollments look like. Review the meaning of each of these. summary(students) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 20.00 21.00 24.00 26.15 30.00 38.00 Mean and Median are measures of center, while the quartiles are measures of spread. One measure of spread missing is the variance or its square root standard deviation: var(students); sqrt(var(students)) ## [1] 28.45 ## [1] 5.333854 The variance is computed by taking the sum of the square differences between the data and the mean and then dividing that by number of items minus one; we will explain why this is what we divide by later in class: \\[ S = \\frac{1}{n-1} \\sum_{j=1}^n (y_j - \\bar{y} )^2 \\] where \\(\\bar{y}\\) is the mean of the sample made up of the \\(\\{ y_1, y_2, \\dots, y_n \\}\\). In the code blocks below I will add some comments about what we are doing: # define a variable for the mean we will consistently use bar for sample mean # again note I use the &lt;- assignment rather than = ybar &lt;- mean(students) # R can then do the arithmetic and when we combine a list of numbers with a single number and square it, it knows what we mean. (students - ybar)^2 ## [1] 78.3225 26.5225 4.6225 9.9225 140.4225 0.7225 37.8225 23.5225 4.6225 14.8225 14.8225 4.6225 37.8225 34.2225 9.9225 3.4225 26.5225 26.5225 14.8225 ## [20] 26.5225 # Then as we said, we sum over all of these and then divide by the number of them # minus 1 sum( (students - ybar)^2 ) / ( length(students) - 1 ) ## [1] 28.45 Note why we compute variance in this way. We are trying to understand how far the data moves from the mean \\(\\bar{y}\\). However we can’t just sum over the differences as that will just add up to zero: sum(students - ybar ) ## [1] 2.842171e-14 We have choices though, we could sum over the absolute value and indeed that does give us a notion of spread. Here for example is the corresponding average for absolute deviation: sum( abs(students - ybar)) / (length(students) - 1 ) ## [1] 4.805263 However the advantage of summing over the squares is that it brings in tools from Calculus that work with quadratic expressions but not with absolute value expressions. We then intend to take an average. For reasons we will justify later, it turns out the correct average to do is to divide by the size of our sample minus 1: \\(n-1\\) is called the degrees of freedom (roughly computing \\(\\bar{y}\\) from the sample has used up one degree of freedom). Note the units on these two statistics: the variance is the average of the sum of the squares (by the number of degrees of freedom) so its units are the square of the units of our sample data (students squared in this case); and then the standard deviation is the square root of the variance and so has units of the original data (students in this case). 2.2 Plots That’s all fine and good, but a picture (or two) is worth a thousand words and probably tens of thousands of computations. Let’s introduce ourselves to two types of plots: 2.2.1 Histograms Histograms are made by binning the data and then drawing bars whose height is the number of data points in each bin. They give us a visualization of the distribution of our data: Taller bars mean more of the data was near those values. hist(students, 10) Note you can adjust the number of bins to use. R tries to guess the labels from the names of the data. You can output the image to a file by right clicking on it, by using the export command in the plot tab in the lower-right quadrant of RStudio or by using a command in the console in the lower-left quadrant. Of course if you are using Markdown to compose your text you don’t need to export the figure, you have it right here. Note the connection to the numerical summaries we have: The mean gives the center of mass of the histogram, while the variance (or standard deviation describes how much it spreads out from there). 2.2.2 BoxPlot Boxplots are a visual representation of the quartile and median statistics for our data: boxplot(students, horizontal=TRUE) # Note the use of = for a parameter instead of &lt;- To interpret boxplots you should think in terms of quartiles: Half of the data is in the box. Half of the data is to either side of the median. 2.3 Categorical Data Example The class sizes data is an example of quantitative data where the information is a numerical quantity that is a feature of the subject. Another type of data used frequently is categorical data, data that gives a description of a feature of the subject. We may sometimes represent categorical data with numbers - for example Male and Female in a data set might be represented with a 0 and a 1; however it would still be categorical describing a feature. There are different types of categorical data depending on the feature: the categories may have an ordering or not; if they have an ordering there may even be a notion of distance between them. It is important to think about what the descriptions for our category mean - If the categories do not have a natural order, or if there is no notion of distance between them, it might be misleading to represent them as numbers and then do computations with the numbers. 2.3.1 Grades The example as teachers you should ponder are grades. What does A, B, C, D, F in a Math, English, History, and Physical Educaiton class mean? Is it valid to replace them with numbers for all of a students classes and compute Grade Point Averages from them? Is it ethical to report that number on a transcript and use it in hiring and admissions decisions? 2.3.2 Example of Categorical Data Note if you are running this code, you will need to download the “Supermarket Transactions.xlsx” file from Github or here. We will also need to use the tidy package. You may need to install this package using install.packages(“tidyverse”). Credit to UC Business Analytics R Programming Guide for this section, you can find the source here https://uc-r.github.io/descriptives_categorical Then you will need to load the libraries: library(readxl) # for reading Excel files library(ggplot2) # for making nice plots - part of the tidyverse supermarket &lt;- read_excel(&quot;Supermarket Transactions.xlsx&quot;, sheet = 2) supermarket ## # A tibble: 14,059 x 16 ## Transaction `Purchase Date` `Customer ID` Gender `Marital Status` Homeowner Children `Annual Income` City `State or Provin… Country `Product Family` `Product Depart… ## &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 2011-12-18 00:00:00 7223 F S Y 2 $30K - $50K Los An… CA USA Food Snack Foods ## 2 2 2011-12-20 00:00:00 7841 M M Y 5 $70K - $90K Los An… CA USA Food Produce ## 3 3 2011-12-21 00:00:00 8374 F M N 2 $50K - $70K Bremer… WA USA Food Snack Foods ## 4 4 2011-12-21 00:00:00 9619 M M Y 3 $30K - $50K Portla… OR USA Food Snacks ## 5 5 2011-12-22 00:00:00 1900 F S Y 3 $130K - $150K Beverl… CA USA Drink Beverages ## 6 6 2011-12-22 00:00:00 6696 F M Y 3 $10K - $30K Beverl… CA USA Food Deli ## 7 7 2011-12-23 00:00:00 9673 M S Y 2 $30K - $50K Salem OR USA Food Frozen Foods ## 8 8 2011-12-25 00:00:00 354 F M Y 2 $150K + Yakima WA USA Food Canned Foods ## 9 9 2011-12-25 00:00:00 1293 M M Y 3 $10K - $30K Bellin… WA USA Non-Consumable Household ## 10 10 2011-12-25 00:00:00 7938 M S N 1 $50K - $70K San Di… CA USA Non-Consumable Health and Hygi… ## # … with 14,049 more rows, and 3 more variables: Product Category &lt;chr&gt;, Units Sold &lt;dbl&gt;, Revenue &lt;dbl&gt; A couple of notes are in order here before we continue: Note that this data set is made up of samples (rows) each of which has 16 features (columns) (well really 15 plus an identifying number in the first column). Some of the features are numerical - Units Sold and Revenue, while others are purely categorical giving us just a description of the value. Also note there is a factor that looks numerical but for which it would probably be misleading to treat it as such. Note that some of the categorical variables have a natural order in the categories like Annual Income while others will not such as State or Province. You will notice the note that the data set has ~14,049 rows: We can only deal with such datasets using a computer program. Without too much work we could find datasets that are too big for Excell and need to be analyzed using a software like R or Python - one reason we teach our students how to use them in their undergraduate courses. 2.3.3 List of features With a large data set it is helpful to list the features we have: colnames(supermarket) ## [1] &quot;Transaction&quot; &quot;Purchase Date&quot; &quot;Customer ID&quot; &quot;Gender&quot; &quot;Marital Status&quot; &quot;Homeowner&quot; &quot;Children&quot; &quot;Annual Income&quot; ## [9] &quot;City&quot; &quot;State or Province&quot; &quot;Country&quot; &quot;Product Family&quot; &quot;Product Department&quot; &quot;Product Category&quot; &quot;Units Sold&quot; &quot;Revenue&quot; We can use sapply to get information about the datatypes R has guessed for each column. sapply(supermarket, class) ## $Transaction ## [1] &quot;numeric&quot; ## ## $`Purchase Date` ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; ## ## $`Customer ID` ## [1] &quot;numeric&quot; ## ## $Gender ## [1] &quot;character&quot; ## ## $`Marital Status` ## [1] &quot;character&quot; ## ## $Homeowner ## [1] &quot;character&quot; ## ## $Children ## [1] &quot;numeric&quot; ## ## $`Annual Income` ## [1] &quot;character&quot; ## ## $City ## [1] &quot;character&quot; ## ## $`State or Province` ## [1] &quot;character&quot; ## ## $Country ## [1] &quot;character&quot; ## ## $`Product Family` ## [1] &quot;character&quot; ## ## $`Product Department` ## [1] &quot;character&quot; ## ## $`Product Category` ## [1] &quot;character&quot; ## ## $`Units Sold` ## [1] &quot;numeric&quot; ## ## $Revenue ## [1] &quot;numeric&quot; Note I say that these are guesses. Depending on the spreadsheet the data came from these may or may not be correct. In particular it is not uncommon for numerical data to be incorrectly typed as character data. R is efficient at cleaning data, however it is a step further than we want to go in this class, just be aware if you or your students start using data sets from the web you will often come upon this issue. 2.3.4 Frequencies We are going to make a contingency table for some of the features of this data. We can count how many members of the sample are in each category of a feature: table(supermarket$Gender) ## ## F M ## 7170 6889 Maybe we want to understand the relationship between the gender of a shopper and their marital status, for this we use a cross classification count: # Note that because the feature Marital Status has a space in it, we have to use quotes to refer to it: table( supermarket$`Marital Status`, supermarket$Gender) ## ## F M ## M 3602 3264 ## S 3568 3625 We can even do multi dimensional tables, though there is a limit to what makes sense. Can you interpret what the 190 in the upper left of this table means? # Here to make this easier to read we are going to do the operation in two separate commands; table1 &lt;- table(supermarket$`Marital Status`, supermarket$Gender, supermarket$`State or Province`) ftable(table1) ## BC CA DF Guerrero Jalisco OR Veracruz WA Yucatan Zacatecas ## ## M F 190 638 188 77 15 510 142 1166 200 476 ## M 197 692 210 94 5 514 108 1160 129 155 ## S F 183 686 175 107 30 607 125 1134 164 357 ## M 239 717 242 105 25 631 89 1107 161 309 2.3.5 Proportions Of course it is useful to think in terms of proportions of the data rather than the raw counts. We can do this by sending the result of a table command through the prop.table. Try it with one of the tables above: table2 &lt;- table(supermarket$`Marital Status`, supermarket$Gender) prop.table(table2) ## ## F M ## M 0.2562060 0.2321644 ## S 0.2537876 0.2578420 Note that we break the procedure into two steps - one making the table and then one producing the proportions with the prop.table() function. The syntax item is in a notebook we need the semicolon. In the console you would just do the first line and then the second line as two separate commands. 2.3.6 Charts Visualizations are straightforward, a typical one would be a bar chart indicating visually the frequency counts: # Note in this one the direction of the quotes matter ggplot(supermarket) + geom_bar(mapping = aes(x=`State or Province`)) + theme(axis.text.x=element_text(angle=45, hjust=1)) Note that the labels, colors, and positions can all be set via commands. ggplot commands always have this structure, one command setting the dataframe to use, one command identifying what to draw, and then commands to change the theme. Using the same theme command for all of your plots in a project means they will all have identical formatting. R is built to make these figures look very nice and there are more options than we will go over - you can explore them with google if you like. 2.4 Exploratory versus Confirmatory Analysis We will spend time in this class doing two types of activity with data. Exploratory Analysis involves looking at a dataset trying to find patterns and relationships through the different features and combinations of those features. The goal in exploration is to develop hypothesis about the population the data is a sample of. Confirmatory Analysis seeks to use data to either confirm or deny a hypothesis - the primary question being asked is Do we have sufficient evidence to support or reject this hypothesis? The major difference is that confirmatory analysis rests upon mathematical models of the situation and then tests the sample against that model. The key factor is how frequently you use a member of the sample. You can use it once to test a hypothesis; but if you return to it again you are now doing an exploration. A big change in the field in the last two decades is that large datasets are becoming very common. With a large data set exploratory analysis can be very fruitful and provided you keep a portion of your data sequestered could even be combined with a confirmatory analysis. 2.5 Correlations and Predictions One of our frequent tasks is to use data to make predictions. The process usually involves exploratory analysis of a data set looking for potential relationships between factors in the sample, and then building models to reproduce features of those relationships. Let’s explore this with the mpg data set, a sample of features for 234 cars. mpg ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compact ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compact ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compact ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compact ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compact ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compact ## 7 audi a4 3.1 2008 6 auto(av) f 18 27 p compact ## 8 audi a4 quattro 1.8 1999 4 manual(m5) 4 18 26 p compact ## 9 audi a4 quattro 1.8 1999 4 auto(l5) 4 16 25 p compact ## 10 audi a4 quattro 2 2008 4 manual(m6) 4 20 28 p compact ## # … with 224 more rows displ for displacement is the total liters the car’s engine has in its cylinders, while hwy is the highway miles-per-gallon efficiency of the vehicle. A basic question is what is the relationship between displacement of the engine and the fuel efficiency. Let’s plot for each engine its fuel efficiency versus the engine displacement. Note that the pattern for producing the graph is similar to the one we used above. ggplot(data = mpg) + geom_point(mapping = aes( x = displ, y = hwy )) Not surprisingly, larger engines are less effecient generally. However the relationship is not exact - why not? Our model for this is going to be that the displacement is known exactly and then the fuel efficiency is given by: \\[ y - y_0 = k (x - x_0) + \\mbox{Error} \\] Where the Error is a random variable encapsulating all of the other features (variables) in the car - its mass, aerodynmaic features, turbo chargers, cargo capacity, transmission, and on and on. It is likely we may never even know all of the variables that have an effect on the fuel efficiency and cause two cars with the same displacement to have different efficiencies and hence the reasonableness of thinking of this as some kind of randomness. Another possibility is that the relationship is not in fact linear. Finally. There may be a subclass of the data that does not fit with the rest. Note the set of six cars with significant displacements (greater than 5 liters) but with fuel efficiencies well above the linear trend. To explore this, we can add more variables to a scatter plot by using color, shape, and size attributes. Lets use color with the class variable in our data: ggplot(data = mpg) + geom_point(mapping = aes( x = displ, y = hwy, color = class )) And indeed we can now make a hypothesis about what is happening with five of those six vehicles. If we remove them from the problems set we would expect our linear model to do even better (by which we mean the Error terms should be smaller and more controllable). 2.6 Motivating Questions I want to end this first lesson with some motivating questions we will be spending our time on this semester. These are questions for which we do not yet have tools to answer but trying to answer them is going to lead us in the technology and tools we build. 2.6.1 Engine Failure An aircraft engine has a 0.1% chance of failing in any given flight-hour of operation. How many flight hours do we expect it to run before its first failure? 2.6.2 Engine Failure 2 We are testing a new aircraft engine so we run four of them continuously until they fail. They last 220, 300, 320, and 350 hours. How many flight-hours do we expect one of these engines to run? 2.6.3 Many Engines Failure An airline is running 655 single enginer aircraft in an hour with the engine used above. How many engine failures do we expect to happen in that hour? 2.6.4 Duplicate Birthdays You enter your classroom on the first day of the fall to a group of 25 students. How likely is it that two of them share the same birthday (day / month)? 2.6.5 Birthday Tickets You stand at the door to an auditorium and as students enter they hand you a ticket with their birthday (day/month). How many students do you expect to have entered before you have your first duplicate? 2.6.6 Accidents An intersection in Greeley, near my house, has on average 1.2 traffic accidents a month. The city, in an attempt to improve on the situation makes a change to the timing of the light. In the month following there are 3 accidents. Do we have evidence that the intersection is now less safe? 2.6.7 Detecting Fraud A common analysis used in forensic accounting, specifically in an attempt to detect if fraud is being attempting is to analyze the frequency with which each digit 0, 1, 2, …, 9 appears in the records. In an honest accounting of a companies finances what digits should be the most common? 2.6.8 Combining Rare Events with Common Events At the scene of a crime the two perpetrators left two samples of blood. One was blood type O, occurring in 60% of the population; while the other was blood type AB occurring in only 3% of the population. A suspect, Oliver has his blood tested and it returns as blood type O. Does this indicate evidence in favor of Oliver committing the crime? Consider this related problem: Your partner draws two cards from a standard 52 card deck and tells you that one is a red face card, and the other is a number card. How likely is it that the 3 of clubs was the number card drawn? 2.6.9 Monty Hall You are on a game show and are presented with three doors, behind one door is a car and behind the other two there are goats. You are asked to choose a door. After you have chosen one the host, Monty Hall (ask your grandparents), opens a second door revealing a goat. He then asks you if you would like to change your selection. What should you do? 2.6.10 Monty-Hilbert Hall You are confronted with a countably infinite number of doors, i.e. they are numbered 1, 2, …, and told that behind one of them is a car and behind the others goats. You are asked to choose a door. After you have chosen one the host, Hilbert, opens all of the other doors but one revealing goats (A LOT of goats). He then asks you if you would like to change your selection. What should you do? 2.6.11 Radioactive Decay The number of atoms of a radioactive isotope in a sample that are likely to decay in a small interval of time is proportional to the number of atoms in the sample. Cesium 137, a by product of some nuclear fission reactors, has a half life of 30 years. It decays, emitting a beta particle, into Barium 137 which has a half-life of just 2.6 minutes and decays emitting a gamma particle - this decay path leading quickly to the release of a gamma particle is what makes Cesium 137 such a dangerous contaminant. What proportion of a sample of Cesium 137 released in an accident do we expect to still be present after 50 years? 2.6.12 Budgets The School of Mathematical Sciences spends on average $5,800 a year on copies. How much should we budget for copies for next year? 2.6.13 Cherry Blossoms The Cherry Trees in Washington DC are a famous gift from the mayor Tokyo in 1914 aimed at improving the cooperation between the US and Japan. Recently they have become possibly evidence of a change in climate around DC. The last few peak blooms (the date at which 70% of the trees in DC are at full bloom) have been on: March 28, March 20, April 1, April 5, March 25, March 25, April 10, April 10, April 9, March 20, March 29, and March 31. Over the entire historical record peak bloom has on average occured on April 3. From the recent blooms, do we have evidence that the average date of peak bloom has changed? 2.6.14 Detecting Fraud in THC Labs This problem comes from the article: https://jcannabisresearch.biomedcentral.com/articles/10.1186/s42238-021-00064-2 about cannabis testing labs in Washington state. There have been some high profile cases of labs in CA, NV, and CO as well as Canada being fine or suspended for falsifying data or for retailers or producers shopping for labs. Something suspicious is happening at some labs in Washington state that test for THC concentrations in consumer sold marajuana. The distribution of concentrations appear to be normal except that there is a discontinuity in the distribution at 20% with a higher density of results just above 20% than just below 20%. Here is the histogram that was found: Whereas here is a distribution of test results from the largest lab in the sate: Suppose that you have a set of results from a lab for concentrations of some samples. How would you go about deciding if they have manipulated the results? (If you would rather not think of this as a THC lab question - Volkswagon was caught doing something very similar with the effeciencies measurments of their TDI vehicles a few years ago). 2.6.14.1 Detecting Fraud I would consider building an entire course around this question alone. 2.6.15 How Many Locomotives A train company numbers its locomotives sequentially 1, 2, 3, …. N You happen to be standing at a station and you observe locomotive number 65 go by. How many locomotives does the company own? 2.6.15.1 The German Tank Problem This is a baby example of the German Tank Problem from World War 2: The allies were collecting maintenance records and recovering damaged tanks which included serial numbers on engines and transmissions. From this data they attempted to estimate how many tanks were being produced in Germany. 2.6.16 Median House Prices Redfin is reporting that the median price of a house in Denver over the last year is $540,000. To what extent is there evidence that something has changed in the Denver real estate market? What are our predictions about future prices? 2.6.17 Healthcare Costs Healthcare costs in the US are made up of three broad categories: Hospitals, Physicians and Clinics, and Prescription Drugs. How are these three costs contributing to the healthcare expenses in the US. What are our predictions about future costs? 2.6.18 What Questions Do You Have? Because this is a course about data, we can look for data related to questions you have. Places I look for data: Kaggle Colorado Data USA Data United Nations Data Pew Charitable Trust The University of California at Irvine Data Science Archives "],["discrete-random-variables-the-binomial-distribution.html", "3 Discrete Random Variables - The Binomial Distribution 3.1 Definitions 3.2 Probability 3.3 Uniform Distribution 3.4 Bernouli Trials 3.5 Binomial Trials 3.6 Cummulative Distributions 3.7 Motivating Problem - The Birthday Paradox 3.8 Expected Value 3.9 Population Variance", " 3 Discrete Random Variables - The Binomial Distribution A motivating problem: Current estimates are that 38% of college and university students report food insecurity (defined as being uncertain they will have enough food for the next month or not knowing when they will next eat). The mathematics major at UNC has 130 students enrolled, how likely is it that 39 or fewer of the students enrolled are food insecure? 3.1 Definitions To get started we will set up some definitions we will use: An experiment is the process by which an observation is made. In our motivating problem the experiment would be inquiring of the mathematics majors whether they are food insecure or not. Experiments on Human Subjects like this one are subject to additional constraints and reviews in order to protect the subjects (subjects need to be protected from harm, their identities need to be protected, and their rights to consent to be included in a study need to be protected). So in this case the actual experiment we do would involve informing the subject of possible harm, taking steps to see that their response is anonymous, and asking them to verify that they are consenting to participate. The sample space associated with an experiment is the set of all possible observations. In our motivating example the sample space would be a list of which students are facing food insecurity and which are not. An event is a set of possible observations from the sample space. 3.1.1 Random Variable For example we are interested, rather than in which students are food insecure, how many students are food insecure. This number: \\(Y\\) the number of mathematics majors at UNC that are food insecure is called a random variable of interest in our problem. Specifying a value or range of values for \\(Y\\) corresponds to an event and identifies a subset of the sample space. Ther eare actually other random variables associated with the same problem that we might be concerned about, but this one is a sort of basic one. 3.2 Probability The probability of an event is the likliehood of that observation occuring from the sample space. What do we mean by likliehood though? We will think of the probability of an even occuring as the proportion of the times it will occur through repetition of the experiment. In some case like rolling dice, flipping coins, or measuring the distance a spring has stretched under a weight it is clear what we mean by repeated observation. However things get a little unclear when we are talking about a situation like our motivating problem. What we mean is: If we were to take groups of 130 students randomly selected from all students throughout the US and ask how many of them are food insecure, how often would we get the particular value of \\(Y\\). Some basic Theorems about Probability from these definitions: Total Probability: The total probability, i.e. the probability that something from the sample space happens must be 1, Valid Probaiblities: no probability can have a value bigger than 1 or less than 0. Complement Probability: If the probability of event A is \\(P(A)\\) then the probability of the complement of event A (all of the members of the sample space that are not in A) is \\(P( \\neg A) = 1 - P(A)\\). Mutually Exclusive Events: Two events are said to be mutually exclusive if their intersection (the cases where both events happen) is impossible i.e. \\(P(A\\cap B) = 0\\). Note this does not necessarily mean that the intersection is empty. Sum of Mutually Exclusive Events: The probability of one of two or both mutually exclusive events happening is the sum of the probabilities of the two events: \\(P(A \\cup B) = P(A) + P(B)\\). Sum of Events: More generally we have the probability of one of two or both events happening is almost coming from the sum, however we must take into account the possiblity of them both happening is being counted in both \\(P(A)\\) and \\(P(B)\\): \\[P(A\\cup B) = P(A) + P(B) - P(A\\cap B) \\] Venn Diagrams: These results should look familiar, they mirror statements about the size of sets A and B, and their union and intersection; with the caveat that P(S) = 0 is not exactly the same as S being empty. 3.2.1 Two Events Happening at Once In this case we will state that two events are inedpendent if the probability that they both happen is the product of the probabilities of either one of them happening: \\[ P(A \\cap B) = P(A) P(B)\\] An example of two independent events would be flipping two coins: getting a heads on one coin flip and getting a heads on the second coin flip. The result of the second flip will not depend upon the result of the first flip (or maybe the right question is, when would the second flip depend on the first one?). An example of two dependent events would be getting a head on the first coin flip of two, and then the flipping of two heads in a row in two coin flips. Note that in order to have two heads the first flip needs to be a head and thus the second event depends on whether the first event has happened or not. It is the dependence causing the probability of one event to change if we know the other event has occured that we are looking for. We will return to the case of dependent events later. 3.3 Uniform Distribution Generally we can define a distribution by just giving the sample space for the random variable, and assigning a probability to each possible value. A few distributions model situations that occur often enough, and have sufficient symmetry that we can compute things for them that they have earned a name. Our first example of that, which will also be the most basic, is the uniform distribution. For the list of possible values our random variable could have, we assume each one is equally likely. The example used frequently comes from rolling dice. For a six sided die, if it is fair, the likliehod of rolling each value is \\(1/6\\). (Note that you can get unfair dice from a magic or toy shop). Find the following probabilities: The likliehood of rolling on die and have a value of 5 or more. The likliehood of rolling two dice and having a value that sums to seven. The likliehod of rolling two dice and having the value of the second one being greater than the value of the first one. 3.3.1 Permutations and Choices To continue with our example we need a couple of constructions from combinatorics: A permutation of \\(n\\) objects is a specific ordering of the objects. For example { “pie”, “bread”, “vegetable”} and {“vegetable”, “pie”, “bread”} are two different permutations of the same 3 objects. The total number of permutations of \\(n\\) objects is given by: \\[ n! = n (n-1) (n-2) \\cdots 3 \\cdot 2 \\cdot 1 \\] Verify this. 3.3.1.1 Example: Find all 3! permutations of {“pie”, “bread”, “vegetable”} 3.3.1.2 Example: Find all 4! permutations of {1, 2, 3, 4} 3.3.1.3 Generalization: Find all the possible permutations of {1, 1, 2, 3} I.e. when two of the elements are identical and so can’t be distinguished. 3.3.1.4 Choices However what we often want to do is ask a question like: In how many ways can we choose 3 cookies from a plate of 5 cookies? In math-ese this is asking: how many subsets of size 3 are there from a set of size 5? 3.3.1.5 Suppose order matters - how many ways with ordering matter can we choose 3 numbers from the set {1, 2, 3, 4, 5} 3.3.1.6 As posed order does not matter: how many ways ignoring order can we choose 3 cookies from a plate of 5 cookies? The answer we find will be: \\[ \\mbox{n choose r} = \\binom{n}{r} = \\frac{n!}{r! (n-r)!} \\] 3.3.1.7 Example: How many ways can we choose 3 objects from a set of 10? 3.4 Bernouli Trials In a Bernouli Trial there is a simple experiment with a sample space consisting of two outcomes {Success, Failure}. Or any other size two descriptors you prefer (Heads / Tails if we are flipping a coin; Red/Black if we are playing roullete; Win/Lose if we are hockey team; etc. ). The probability of Success is given by p and the probability of failure is given by 1-p.  Flipping a fair coin we expected to get a HEADS half of all flips. Note This is a statement about long term behavior. For example it means if we flip the coin one thousand times we would expect to have close to 500 HEADS. A common misconception: If we flip a coin twice we expect to see two HEADS sometimes. A common misconception: If we flip a coin two hundred times, we expect to see sequences of just heads. Let’s try it! The sample command in R produces a sample from a given column. By defining a column to be {“H”, “T”} we can then simulate flipping a coin (much faster than actually flipping the coin) by using sample. Sample takes a parameter replace that identifies whether we are returning members to the set after selecting them. Because we are simulating flipping a coin repeatedly we want to replace the value after each selection. x &lt;- c(&quot;H&quot;, &quot;T&quot;) result &lt;- sample(x, 200, replace=TRUE) result ## [1] &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; ## [43] &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; ## [85] &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; ## [127] &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; ## [169] &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; You can also pass the sample command a list of probabilities for each element of x; note that order matters so check it. For a fair coin the default of each outcome being equally likely works. However note that unfair coins are not unheard of - you can purchase them at magic and toy shops. A typical human trying to fake coin flips will distribute them too uniformly and will not have as many long sequences of “H” as we expect to see from an actual experiment. 3.5 Binomial Trials We now have all the tools for the question at hand. Let’s start with simulating the question using sample. Randomly selecting an individual student from the population of US college students and asking whether they are food insecure or not is a Bernouli trial with a probably of 0.38 that the student is Food Insecure; and a probability of \\(1-0.38 = 0.62\\) that they are not. Specifically the result for each student is independent of the others (the probabilities 0.38 and 0.62 in the trial are not changing). We will use the prob parameter and pass it a column of the two probabilities. x &lt;- c(1, 0) result &lt;- sample(x, 130, replace = TRUE, prob=c(0.38, 0.62) ) result ## [1] 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 0 ## [86] 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 You may notice that rather than using strings for the entries in x I used 1 and 0. The reason is that what we care about is the number of students that are food insecure, this is our random variable of interest and now we can compute its value by just summing up the numbers in result the value of which will be the number of 1s. sum(result) ## [1] 47 Because the sample command uses (pseudo)-random numbers, if we run this again we will get a different result (also note each time I compile this textbook the values change). Do it a few times yourself and see what we get. sum(sample(x, 130, replace=TRUE, prob=c(0.38, 0.62))) ## [1] 50 This is a little bit tedious to run a bunch of times and we also should be keeping track of the results. For example, what is the smallest value you have seen so far? Luckily, one of the things computers are absolutely brilliant at is doing the same thing over and over and keeping track of the results. Let me introduce you to one of the two real programming structures we will use in this class: a for loop. A for loop tells the computer to complete the same set of commands multiple times. # We are going to keep track of the results in a variable called trials, so we first initalize this variable as an empty column trials &lt;- c() # Then our for loop - in this case we just want the loop to repeat the command 100 times for (k in 1:100) { # each loop will compute the number of food insecure students in our random sample of 130 students, # make that a column, and # then add that to the end of the trials column trials &lt;- c(trials, sum(sample(x, 130, replace=TRUE, prob=c(0.38, 0.62)))) } trials ## [1] 52 47 53 55 48 44 50 47 54 48 47 57 49 54 51 50 56 50 65 36 45 49 47 43 48 55 51 43 53 44 51 49 46 56 45 52 47 43 49 51 50 57 48 43 56 52 49 49 43 41 51 43 64 42 58 53 ## [57] 51 46 48 56 42 46 51 50 50 50 48 36 44 48 50 46 50 51 46 52 60 45 47 45 53 50 53 43 45 36 47 55 47 55 56 45 37 46 46 56 47 48 51 43 Let’s use a historgram to look at the result: # The paramter breaks is determining how many bins to use hist(trials, breaks=10) summary(trials) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 36.00 46.00 49.00 48.96 52.00 65.00 So interestingly the minimum value is not very small. What does this tell us about the number of mathematics majors we might see that are food insecure? What would our interpretation be if we found only 10 of the mathematics majors were food insecure? Finally you can probably tell me you are not surprised that the mean is close to 0.38 * 130 ## [1] 49.4 we will make your instinct about this precise! 3.5.1 Plan Let me pause here and say that this is our plan for the course. We will take a given problem and experiment with it, using R to simmulate things like flipping coins or rolling dice, or as in this example selecting random college students and asking them whether they are food insecure or not. Then we will ask the theoretical questions where we try and model the situation. 3.5.2 Binomial Distribution Now let’s develop a model of what is happening. To do this we will ask the following question: For Y, the random variable given by the number of success in a set of 130 Bernouli trials, what is \\(P(Y = r)\\) for a given integer r? We could think of this as a function of \\(r\\) and as such it is called the Probability Distribution of the random variable \\(Y\\). In this case the \\(Y\\) takes discrete values and so we would call \\(P(Y=r)\\) a discrete probability distribution. Find the following probabilities: \\(P(Y=r)\\) for r &lt; 0 \\(P(Y=r)\\) for r not an integer \\(P(Y=r)\\) for r &gt; 130 Okay so now lets fix a particular r, and being mathematicians lets start simple with r = 1: \\(P(Y=1)\\) is the probability that one of the 130 students is food insecure. There 130 different students who could be the one that is food insecure; and then the probability of having one success is given by 0.38; while the probability of 129 failures is \\(0.62^{129}\\). Thus the solution is: \\[ P(Y=1) = 130 \\cdot 0.38 \\cdot 0.62^{129} \\] Specifically the 130 factor is because mutually exclusive events sum their probabilities; and luckily for us each one is equally likely so we just get the multiple of the number of events. The $0.38 ^{129} $ factor is because independent events multiply their probabilities. Note that this depends on the individual trials being independent. For the food insecurity quesiton, is this reasonable? We get: 130 * 0.38 * 0.62^129 ## [1] 8.170618e-26 \\(P(Y=2)\\) in the case of two students out of the 130 being food insecure: There are \\[ \\binom{130}{2} \\] ways to select those two students. Then the likelihood that those two are success is \\(0.38^2\\); and the likliehood the other 128 are failures is \\(0.62^{128}\\). Putting it all together we have: \\[ P(Y=2) = \\binom{130}{2} 0.38^2 \\cdot 0.62^{128} \\] We get: choose(130, 2) * 0.38^2 * 0.62^128 ## [1] 3.23003e-24 More generally we find: \\[ P(Y=r) = \\binom{130}{r} 0.38^r \\cdot 0.62^{130 - r} \\] Plotting the whole range of values: r &lt;- c(0:130) v &lt;- choose(130, r) * 0.38^r * 0.62^(130 - r) plot(r, v) 3.5.3 Binomial Distribution More generally: A Binomial Trial is made up of n identical indepedent Bernouli Trials each with probability p of success and 1-p of failure. The random variable \\[Y = \\mbox{The number of successes in the n trials} \\] has a probability distribution function given by: \\[ P(Y = r) = \\binom{n}{r} p^r (1-p)^{n-r} \\] Vary the p and the n a bit in the plot below and see how the distribution changes: p &lt;- 0.01 n &lt;- 30 r &lt;- c(0:n) v &lt;- choose(n, r) * p^r * (1-p)^(n - r) plot(r, v) 3.5.4 Checking a few things So a few things to check before we move on. The total probability should be 1. It’s not clear it will be: sum(v) ## [1] 1 It is interesting to ask why it works this way. You might recall that you have seen the binomial coefficient before, and in fact it even gets its name from the following problem: \\[ (a + b)^n = \\sum_{r=0}^n \\binom{n}{r} a^r b^{n-r} \\] Now suppose \\(a=p\\) and \\(b=1-p\\); what do we learn? No probability should have a value less than 0; or greater than 1. 3.6 Cummulative Distributions The motivating problem question was: How likely is it that out of the 130 mathematics majors 39 or fewer of them are food insecure. In terms of probability this quesiton is asking: \\(P(Y \\leq 39)\\) And note we can compute that by just taking a sum: \\[ P(Y \\leq 39) = \\sum_{r=0}^{39} P(Y = r) = \\sum_{r=0}^{39} \\binom{130}{r} 0.38^r \\cdot 0.62^{130-r} \\] We could derive a formula for expressions right this, and if you enjoy algebra it is not a bad exercise. Do you enjoy algebra? I frankly do not. Using a computer we can just compute it: r &lt;- c(0:39) sum( choose(130, r) * 0.38^r * 0.62^(130 - r) ) ## [1] 0.03528625 This is a good place to introduce some hard coded functions for our first named distribution: The Probability Distribution Function for the Binomial Distribution is given by the dbinom(r, n, p) command: dbinom(39, 130, 0.38) ## [1] 0.01228386 The Cummulative Distribution Function for the Binomial Distribution is given by the pbinom(r, n, p) command: pbinom(39, 130, 0.38) ## [1] 0.03528625 This us of d and p together with the name of the distribution will continue through all the distributions we work with. 3.6.1 Conclusion to the Motivating Question So at this point we can conclude our motivating question: What we have learned is that in a randomly selected group of 130 US college students the probability that we would have 39 or fewer of them indicating they are food insecure is less than 5%. We actually have the exact value or can at least get very good approximations of it. However in our class, and particularly in Applied Statistics, we are actually not that concerned with the exact value. We are more interested in bounding the result above and below. In fact in applications in the real world, if your answer depends on the exact value coming from your model it should set off alarm bells. In this case again we are seeing that the likliehood of 39 or fewer students with food insecurity out of 130 randomly selected US college students is small. If we had observed 39 we would then be inclined to believe that the other possibility is occurring: Our assumption is false. Meaning in this case Mathematics Majors at UNC are not a random sample of US college students - there are lots of reasons, once we have evidence of it, we might realize are contributing to them not being random (the primary one being that there is in fact a stark difference between the food insecurity of 2-year students and 4-year students). The other possibility is that one of the other assumptions is false - perhaps the students are not well represented as independent trials. 3.6.2 General Cummulative Distribution Functions Its worth spending some more time on cummulative distribution functions, lets look at them in general case of a Binomial Trial with n trials with a probability of success of each trial of p. n &lt;- 130 p &lt;- 0.38 r &lt;- c(-1:(10*n))/10 plot(r, pbinom(r, n, p), type=&quot;l&quot;) Note: We only plotted the distribution function at integer values of r because it is zero everywhere else. However the cumulative distribution function gives the probability that \\(Y \\leq r\\) and so it has a step like behavior. Note: I generally LOVE using software to draw plots as I am an awful artist - most of my low grades in math classes like trigonometry and geometry in high school were because of my drawings. However one place where software consistently messes us up is in drawing step functions - this picture would be better without the vertical lines connecting the horizontal segments. Vary p and n to see how the plot changes. In particular for small values of n the steps should become more prominent. You might have to increase the 10 in the code snippet to get the mesh to be finer - if you notice the vertical segments are not looking very vertical this is what should be changed. 3.7 Motivating Problem - The Birthday Paradox In a class of 25 students, how likely is it that two or more of them share the same birthday (day / month)? This seems like it might be a Binomial Trial; however are the trials identical and independent? Question: write a birthday problem that will be a Binomial Trial. Explain why in that case the trials are identical and independent. 3.8 Expected Value The expected value of a quantitative random variable is defined to be the mean of the values of the variable over a large number of repeated trials of the variable. Given a discrete probability distribution for a random variable \\(Y\\): \\(p(r) = P(Y=r)\\) we can compute the expected value of \\(Y\\) by taking the weighted average of all possible values of \\(r\\) weighted by the likliehood of that r: \\[ E(Y) = \\sum_{r: p(r) \\neq 0} r p(r) \\] 3.8.1 The Expected Number of Students Experiencing Food Insecurity Returning to our problem. If we have a group of 130 randomly selected US college students, the likliehood that any one of them is experiencing food insecurity is 0.38; and assuming that they are independently selected the number of ones in the 130 that are experiencing food insecurity fits the Binomial Distribution. Thus we can compute the expected number by summing over r = 0, 1, 2, …, 130 with each r weighted by \\[ \\binom{130}{r} 0.38^r \\cdot 0.62^{130 - r} \\] Using R we get: r &lt;- c(0:130) sum( r*dbinom(r, 130, 0.38)) ## [1] 49.4 It is probably not shocking that this is the same number we saw above for \\(130 \\cdot 0.38 = 49.4\\) and that it is also close to the mean we got in our sampling experiment we ran above. 3.8.1.1 Expected Value of a Binomial Distribution We can show that this always happens. Note that in general this is a class that I want to have focus on computations - so while we are going to give the exact proofs of some of our results like this one, we will do them as a class rather than have you work on them as excersises. I do think that this computations could be used to motivate some problems you have students do in an algebra or pre-calculus or calculus class, but they are sort of parallel but not part of the goals for our class. Let \\(Y\\) be a binomial distributed random variable for n trials with probability of success in each trial p. Define \\(q = 1-p\\). Then we have \\[ E(Y) = \\sum_{r=0}^n r \\binom{n}{r} p^r q^{n-r} \\] Note that the first term in this sum is in fact 0 and so we could index the sum from 1; then expand the binomial coefficient: \\[ E(Y) = \\sum_{r=1}^n \\frac{n!}{(r-1)! (n-r)!} p^r q^{n-r} \\] We then notice that the terms in this sum are themselves close to being binomial probabilities for a different number of trials. For example what happens if we factor out \\(np\\) from each term: \\[ E(Y) = np \\sum_{r=1}^n \\frac{(n-1)!}{ (r-1)! (n-1 - (r-1))! } p^{r-1} q^{n-1 - (r-1) } \\] and finally we note that we could re-base the index of the sum back to 0: \\[ E(Y) = np \\sum_{s = 0}^{n-1} \\binom{n-1}{s} p^s q^{n-1 -s} \\] and because the sum is now a sum over all of the probabilities of a binomial distribution it gives 1 and so: \\(E(Y) = np\\). Which is EXACTLY what your instinct already told you we should get!!!! 3.8.2 General Expected Values First we note that if \\(Y\\) is a random variable then expressions using \\(Y\\) are themselves random variables - though almost certainly with different distributions. Specifically given a function \\(f(r)\\) such that every possible value of \\(Y\\) is in the domain of \\(f\\) we have: \\[ E(f(Y)) = \\sum_{r : p(r) \\neq 0} f(r) p(r) \\] 3.8.3 Examples Find the following Expected Value: If \\(A\\) and \\(B\\) are random variables show that \\(E(A + B) = E(A) + E(B)\\) If c is a number show that \\(E( c A) = c E(A)\\) Show that for the constant random variable 1 (i.e. the value is 1 with probability 1) E(1) = 1. Use 1-4 to show that \\(E(A - E(A)) = 0\\) 3.9 Population Variance We have just shown that the expected value of \\(Y\\) is \\(np\\). Now what we would like to understand is the extent to which \\(Y\\) gives us values smaller than or lesser than \\(np\\). The two related metrics we will frequently use to study spread in this class are the variance and its square root, the standard deviation of the variable. Note that there are other notions of spread that are useful, particularly the use of quartiles and percentiles is another way to quantify the spread of a random variable or data from its center. We also are going to (try to consistently) draw a distinction between the population variance and the sample variance. The one is a statement about how the random variable spreads for the whole population; while the other is a measurement of the variance specific to that sample. Later on we will show that the two are connected, but also that they are absolutely not the same things. Let \\(\\mu = E(Y)\\) by the expected value of the random variable \\(Y\\), or in other words the population mean. The Population Variance is defined to be the expected value: \\[ \\sigma^2 = V(Y) = E( (Y- \\mu)^2 ) \\] We call this \\(\\sigma^2\\) because it will have the units of \\(Y^2\\) and its square root, \\(\\sigma\\) is the population standard deviation that will have the same units as \\(Y\\). 3.9.1 Methods of Computing \\(V(Y)\\) We can do a bit of algebr and show that \\[ \\sigma^2 = V(Y) = E(Y^2 ) - E(Y)^2 \\] Note that this implies that \\(E(Y^2) \\geq E(Y)^2\\). 3.9.2 Variance of a Binomial Random Variable So we can find the variance of our binomial random variable, either for an explicit case: r &lt;- c(0:130) sigma2 &lt;- sum( r^2*dbinom(r, 130, 0.38)) - (130*0.38)^2 sigma2 ## [1] 30.628 Though note that usually the square root is more useful: sqrt(sigma2) ## [1] 5.534257 Implying a variation of 5.5 students from the mean of 49.4. We will get to specifically what this tells us later, for now it is enough to say that the larger this value the more we expected to see the random variable return values further from the mean. 3.9.2.1 Exact Value We can also do some algebra to find the exact value. Let \\(Y\\) be a binomial random variable for n trials with the probability of success of each trial being p. Define \\(q = 1-p\\): \\[ V(Y) = E(Y^2) - (np)^2 \\] So we need to find \\[ E(Y^2) = \\sum_{r=0}^n r^2 \\binom{n}{r} p^r q^{n-r} \\] Not very useful as while r is a factor of r!, \\(r^2\\) is not. Note we could work with \\(E(Y(Y-1)) + np = E(Y^2)\\) instead and then this will play well with the factorial (and the sum as it turns out): \\[ E(Y(Y-1)) = \\sum_{r=0}^n r (r-1) \\binom{n}{r} p^r q^{n-r} \\] Noting that the first two terms in the sum are zero and expanding out the binomial coefficient in factorials we have: \\[ E(Y(Y-1)) = \\sum_{r=2}^n \\frac{ n!}{ (r-2)! (n-r)! } p^r q^{n-r} \\] If we factor out a \\(n (n-1) p^2\\) and rebasing the sum (using exactly the same pattern as above) we get: \\[ E(Y(Y-1)) = n (n-1) p^2 \\sum_{s=0}^{n-2} \\binom{n-2}{s} p^s q^{n-2-s} \\] The sum is now a sum over a binomial distribution with \\(n-2\\) trials and so adds up to 1; and thus we have: \\(E(Y(Y-1)) = n (n-1) p^2\\) Putting it all together we have: \\[ V(Y) = n (n-1) p^2 + np - (np)^2 = \\mbox{algebra} = n p (1-p) \\] It is interesting to fix \\(n\\) and plot this as a function of p (it is simple enough to do by hand, but we have R sitting right here): n &lt;- 130 p &lt;- c(0:1000)/1000 v &lt;- n*p*(1-p) plot(p, v, type=&quot;l&quot;) "],["geometric-and-poisson-distributions.html", "4 Geometric and Poisson Distributions 4.1 Geometric Distributions 4.2 Poisson Distributions 4.3 Using the Cummulative Distribution 4.4 Moments and Moment Generating Functions 4.5 What have we skipped?", " 4 Geometric and Poisson Distributions We can use the Bernouli Trials we met in the last chapter to build two other distributions that describe common situations (and also have enough symmetry they are computable). 4.1 Geometric Distributions This question worked better in Texas than it does in Colorado because of mail in voting, but still: We are trying to quickly estimate what proportion of voters in Greeley are casting their ballots for candidate A running for mayor of the city. We do this by standing outside a voting location and asking candidates whether they voted for A or B and counting how many people we have to ask before we find our first person voting for A. Say for example that the 4th person we ask says they are voting for candidate A. Let’s see if we can use that number to estimate what proportion of the population p is voting for A. Assuming that our sample is random (and note that’s the problem with Colorado voting) we could think of each time we ask a voter leaving the polling location who they voted for as a Bernouli trial with a probability of success p (granted we don’t know p, but that’s why we study math!). So the question becomes: If Y is the number of times we had to conduct our Bernouli trial before we got our first success, what is \\(P(Y=4)\\)? Note that in order for trial 4 to be our first success 4 things had to happen - we had to have 3 trials in succession that were failures, each with a probability of 1-p; and then we had to have one trial that was a success with probability p. Provided that these events are independent then the likliehood of all 4 of them happening is just the product of these 4 factors: \\[ (1-p)^3 p \\] More generally the geometric distribution has a probability distribution function given by, for \\(r\\geq 0\\): \\[ P(Y=r+1) = (1-p)^{r} p \\] This is fundamentally different from our Binomial Distribution. For the binomial distribution with \\(n\\) trials the largest the random variable could be is \\(n\\). In this case, for our geometric distribution \\(Y\\) is potentially unbounded. For every \\(r\\) the value of \\(P(Y=r+1)\\) is non-zero (though possibly very small). It’s helpful to fix a \\(p\\) and plot the probabilities of \\(Y=r+1\\) for the first few r. The thing to notice in the plot is that in order for the random variable to have a chance of getting large, p needs to be small so that there is a large probability of the failures in the trial stacking up. Depending on the value of p you put in, you may need to extend the values r is using by adjusting the 25. p &lt;- 0.51 r &lt;- c(0:25) v &lt;- (1-p)^(r) * p plot(r, v) So for example: If candidate A has 0.51 of the population supporting them, the probability that we needed to ask 4 voters before we found the first one voting for candidate A is: (1-0.51)^3 * 0.51 ## [1] 0.06000099 Meaning if we did this experiment at 100 polling sites or 100 different times, 6 of those times we would expect to wait until the 4th voter. But wait there is more. What we really care about is the complement of the cummulative distribution: What is the probability that we had to wait until the 4th or LATER voter to find the first vote for A. This is a better way to ask because we are really asking how likely is it that candidate A with 0.51 of the vote, is doing this badly OR worse in the voters we have talked to. Note that the complement of this event is: What is the probability that we found the first voter for A in 0, 1, 2, or 3 voters questioned? We can than subtract that number from 1 to answer the question above. c &lt;- (1-0.51)^0 * 0.51 + (1-0.51)^1 * 0.51 + (1-0.51)^2 * 0.51 c ## [1] 0.882351 R has a cumulative distribution for the geometric distribution coded in pgeom (note that there is an annoying thing in R where its definition of the geometric distribution is off by 1 from others): pgeom(2, 0.51) ## [1] 0.882351 and then subtracking this from 1 we get: 1 - pgeom(2, 0.51) ## [1] 0.117649 Meaning that even for a candidate with 0.51 of the vote, we are not overly surprised when we have to talk to 4 voters before we find a supporter. It is interesting to ask, how does this probability change as p changes: p &lt;- c(0:1000)/1000 v &lt;- 1 - pgeom(2, p) ## Warning in pgeom(2, p): NaNs produced plot(p, v, type=&quot;l&quot;) 4.1.1 Expected Value The expected value can be computed exactly, and in this case we sort of prefer to do it that way as note that the one thing computers are not great at is infinite sums and infinite integrals. However the values of the distribution decay exponentially and so we actually do expect sums over the distribution to converge quickly. So we can find the approximate expected value by computing \\[ \\sum_{r=0}^N (r+1) p(r) \\] for a medium to large value of \\(N\\). N &lt;- 100 r &lt;- c(0:N) p &lt;- 0.51 sum( (r+1)*(1-p)^r * p ) ## [1] 1.960784 Can you make a prediction about the exact value? 4.1.1.1 Exact Value To compute the exact value of the sum, we just do it, using our one trick for adding up infinite sums - write it in terms of a geometric series. \\[ E(Y) = \\sum_{r=0}^\\infty (r+1) q^r p = p \\frac{d}{dq} \\sum_{r=0}^\\infty q^{r+1} \\] The sum is now a geometric series and we have a formula for its result: \\[ E(Y) = p \\frac{d}{dq} \\left[ \\frac{q}{1-q} \\right] = \\mbox{algebra/calculus or calcugebra} = \\frac{1}{p} \\] 4.1.2 Variance The approximate variance can be found using the formula \\(\\sigma^2 = V(Y) = E(Y^2) - \\mu^2\\) with \\(\\mu = E(Y)\\). For example: N &lt;- 100 r &lt;- c(0:N) p &lt;- 0.51 sum( (r+1)^2 * (1-p)^r * p ) - 1/p^2 ## [1] 1.883891 4.1.2.1 Exact Value To compute the exact value of the sum: \\[ \\sigma^2 = V(Y) = E(Y^2) - \\frac{1}{p^2} = E(Y (Y+1) ) - \\frac{1}{p} - \\frac{1}{p^2} \\] So we look for: \\[ E(Y (Y+1) ) = \\sum_{r=0}^\\infty (r+1) (r+2) q^r p = p \\frac{d^2}{dq^2} \\sum_{r=0}^\\infty q^{r+2} = p \\frac{d^2}{dq^2} \\frac{q^2}{1-q} \\] Some Calculus and Algebra later: \\[ E(Y(Y+1)) = \\frac{2}{p^2} \\] Putting it all together: \\[ V(Y) = \\frac{2}{p^2} - \\frac{1}{p} - \\frac{1}{p^2} = \\frac{1-p}{p^2} \\] 4.2 Poisson Distributions Consider a particular intersection. We are interested in understanding the number of accidents that will happen at this intersection in a week. One way to think about this is to consider that if we break our units of time down to a small enough amount of time that only one accident can happen at a time - say we consider individual minutes. Now that only one accident is possible we could consider \\(p\\) the probability that an accident occurs in any given minute at this intersection: \\[ P( \\mbox{an accident occurs in a minute}) = p\\] \\[ P( \\mbox{no accident occurs in a minute}) = 1- p \\] \\[ P( \\mbox{more than one accident occurs in a minute}) = 0 \\] We then take smaller and smaller units of time. Our implicit assumption here is each subunit of time is an identically distributed independent Bernouli trial. This is probably a shaky assumption, but lets proceed and see what happens. So for a subunit with \\(n\\) occurences in a week we have: \\[ P(Y = r) \\sim \\binom{n}{r} p^r (1-p)^{n - r} \\] We now want to take a limit as \\(n\\to \\infty\\), i.e. as the size of the subunits of time become smaller and smaller. Of course in doing this the probability \\(p\\) will change. Let’s assume that it just scales linearly: i.e. the probability of an accident in a given minute is 60 times the probability of an accident in a given second. This is a reasonable assumption if: Everything for a given subunit size is IID; and the subunits are small enough that only one accident can happen at a time. Let \\(p = n \\lambda\\) for some parameter \\(\\lambda\\) and then we take the limit of \\(P(Y=r)\\) above: \\[ P(Y=r) = \\lim_{n\\to \\infty} \\binom{n}{r} \\left( \\frac{\\lambda}{n} \\right)^r \\left( 1 - \\frac{\\lambda}{n} \\right)^{n-r} \\] We expand out the binomial coefficient: \\[ = \\lim_{n\\to \\infty} \\frac{n (n-1) \\cdots (n-r+1)}{r!} \\left( \\frac{\\lambda}{n} \\right)^r \\left( 1 - \\frac{\\lambda}{n} \\right)^{n-r} \\] factor the \\(\\lambda/n\\) and \\(1- \\lambda/n\\) terms apart and recombine: \\[ = \\lim_{n\\to \\infty} \\frac{\\lambda^r}{r!} \\left(1 - \\frac{\\lambda}{n} \\right)^n \\cdot \\frac{ n (n-1) \\cdots (n-r+1)}{n^r} \\left(1 - \\frac{\\lambda}{n}\\right)^{-r} \\] expand the numerator of the fraction and factor out the parts not depending on \\(n\\): \\[ = \\frac{\\lambda^r}{r!} \\lim_{n\\to \\infty} \\left(1 - \\frac{\\lambda}{n}\\right)^n \\left(1 - \\frac{\\lambda}{n}\\right)^{-r} \\left(1 - \\frac{1}{n}\\right) \\left( 1 - \\frac{2}{n} \\right) \\cdots \\left( 1- \\frac{r-1}{n} \\right) \\] Now inside the limit, all but the first factor only depends on \\(n\\) inside of the parenthesis; each of this is going to 1. We also recall that \\[ \\lim_{n\\to \\infty} \\left(1 - \\frac{\\lambda}{n}\\right)^n = e^{-\\lambda} \\] and putting it all back together we have: \\[ P(Y=r) = \\frac{\\lambda^r}{r!} e^{-\\lambda} \\] 4.2.1 What have we learned? So we now have a definition: A random variable \\(Y\\) is called a Poisson Process if it is coming from a trial that has the following property \\(Y\\) is the number of events happening in a unit of time, distance, area, volume or any thing which can be subdivide into smaller and smaller pieces which has the propeerty that for small enough subdivisions the events can only happen once and the likliehood that they happen becomes an identical independent Bernoulie trial. Interestingly, small deviations from this assumption will still yield a good approximation. The probability distribution of a Poisson Process satisfies the probability distribution for \\(r \\geq 0\\): \\[P(Y=r) = \\frac{\\lambda^r}{r!} e^{-\\lambda} \\] with some parameter \\(\\lambda &gt; 0\\). 4.2.1.1 Plotting the Distribution This distribution is coded in R as dpois, lets plot the distribution for the first few \\(r\\) with a given \\(\\lambda\\): lambda &lt;- 1.1 r &lt;- c(0:100) v &lt;- dpois(r, lambda) plot(r, v) What is the effect of increasing \\(\\lambda\\)? Decreasing \\(\\lambda\\)? 4.2.1.2 Cummulative Distribution The cumulative distribution, the probability that \\(Y \\leq r\\), can be plotted, it is coded in R as ppois: lambda &lt;- 1.1 n &lt;- 100 # The upper bound on n for our plot r &lt;- c(-1:(10*n))/10 plot(r, ppois(r, lambda), type=&quot;l&quot;) Again what is the effect of changing \\(\\lambda\\)? Make a prediction before you draw the graph. Again you may have to increase the fineness of our mesh in order for the graph to render correctly. 4.2.2 Expected Value So let’s suppose we have found that \\(\\lambda = 1.1\\) for our intersection (we will return to finding \\(\\lambda\\) from data later). How many accidents do we expect to see each week? I.e. over the long term if we take the mean of the number of accidents what will we find? Using R we can approximate it: r &lt;- c(0:100) lambda &lt;- 1.1 sum( r*dpois(r, lambda) ) ## [1] 1.1 Which seems well…. clearly we should be able to show that this will always happen! \\[ E(Y) = \\sum_{r =0}^\\infty r P(Y=r) = \\sum_{r=0}^\\infty r \\frac{\\lambda^r}{r!} e^{-\\lambda} \\] Again I will note here that the distributions for which we have names are the ones for which the mathematical structure tends to give us formulas with which we can actually do something. We will talk as the course goes on about why this works, and what we can do more generally. Continuing: \\[ E(Y) = e^{-\\lambda} \\sum_{r=1}^\\infty \\frac{\\lambda^r}{(r-1)!} \\] \\[ = \\lambda e^{-\\lambda} \\sum_{s=0}^\\infty \\frac{\\lambda^s}{s!} = \\lambda e^{-\\lambda} e^{\\lambda} = \\lambda\\] So the \\(\\lambda\\) parameter for our Poisson Distribution is exactly the expected value of the random variable. 4.2.3 Variance of the Poisson Random Variable We can also compute the variance, using again our shortcut: \\(V(Y) = E(Y^2) - \\mu^2\\) where \\(\\mu = E(Y)\\). r &lt;- c(0:100) lambda &lt;- 1.1 sum( r^2*dpois(r, lambda) ) - lambda^2 ## [1] 1.1 I would forgive you if you are wondering if maybe we just made a mistake! As before the trick to use is that \\(E(Y^2) = E(Y(Y-1)) + E(Y)\\) \\[ E(Y(Y-1)) = \\sum_{r=0}^\\infty r (r-1) \\frac{\\lambda^r}{r!} e^{-\\lambda} = e^{-\\lambda} \\sum_{r=2}^\\infty \\frac{\\lambda}{(r-2)!} \\] and we find \\[ = \\lambda^2 e^{-\\lambda} \\sum_{s=0}^\\infty \\frac{\\lambda^s}{s!} = \\lambda^2 \\] Putting it together we have: \\[ V(Y) = \\lambda^2 + \\lambda - \\lambda^2 = \\lambda\\] So in summary a Poisson random variable with parameter \\(\\lambda\\) has expected value and variance \\(\\lambda\\). 4.3 Using the Cummulative Distribution Consider our dangerous intersection that we now know has 1.1 accidents per week. Find an upper bound for the number of accidents we will have 95% of the time. Mean we want to find the smallest \\(r\\) such that \\(P(Y \\leq r) \\geq 0.95\\). The qpois function is the inverse Cummulative Distribution Function. It takes a probability \\(0\\leq q \\leq 1\\) and returns the value \\(r\\) such that \\(P(Y\\leq r) = q\\). qpois(0.95, 1.1) ## [1] 3 Note that because the probability is discrete the result is not that the CDF is exactly 0.95 here: ppois(3, 1.1) ## [1] 0.9742582 Meanwhile, the likliehood that we will go a week with 3 or more accidents is \\(P(Y \\geq 3)\\): 1 - ppois(2, 1.1) ## [1] 0.09958372 For a given value of \\(\\lambda\\) we can play variations on these games for any number of combinations. 4.4 Moments and Moment Generating Functions One idea which we will need later in class and we should introduce now is that of finding moments. We have seen that Expected Value and Variance have uses in characterizing a distribution, and for distributions with sufficient symmetry are things we can compute exactly. In an effort to characeterize distributions more broadly one is then led to consider the question of moments. The \\(m\\)’th moment of a distribution is defined to be: \\[ E(Y^m) \\] We then define the Moment Generating Function of a random variable to be: \\[ M(t) = E(e^{t Y} ) \\] provided that it exists. This generates moments in the sense that provided that \\(t\\) is in an open interval where \\(M(t)\\) exists we have: \\[ E(Y^k) = \\frac{d^k}{dt^k} M(t) \\bigg|_{t=0} \\] Two random variables with moment generating functions that exist and are equal are identical. Meaning that moment generating functions, if they exist, will uniquely characterize a distribution. This is a fact we will use once in this class in a few weeks. 4.4.1 Moment Generating for a Poisson Distribution Let’s compute the moment generating function of the Poisson Distribution: \\[ M(t) = \\sum_{r=0}^\\infty e^{t r - \\lambda} \\frac{\\lambda^r}{r!} = e^{-\\lambda} \\sum_{r=0}^\\infty \\frac{\\left( \\lambda e^{t} \\right)^r}{r!} = e^{\\lambda (e^t - 1)} \\] If you go to the wikipedia page for any of our named distributions you will find their moment generating functions given. An interesting question we unfortunately do not have time for is what the set of values of \\(t\\) for which \\(M(t)\\) exists tells us about the distribution. 4.5 What have we skipped? There are couple of things we have carefully ignored. The main one being: While the Geometric and Poisson distributions are examples of how we can have a discrete random variable with infinitely many allowed values but with a distribution we can compute with; one might be wondering how can we work with an infinite discrete random varialbe that is for example uniform: You are confronted with infinitely many doors, one of which has a car behind it and the rest of them goats. Can you choose one door at random with each door being equally likely that you will choose it? If you do so, how likely is it that you will find the car rather than a goat? For that matter, suppose you want to select a random integer to use when a date asks you “What is your favorite Integer?” What are the chances you would select 1001? "],["continuous-random-variables.html", "5 Continuous Random Variables 5.1 Uniform Distribution", " 5 Continuous Random Variables We spent the last two days thinking about discrete probability distributions. Particularly these include random variables that are counting things (what do the binomial, geometric, and Poisson random variables count?). However you can probably give some examples of numerical random variables which are not discrete. Note the thing that should give us pause, and explains why we have to introduce these random variables in a slightly different way from how we introduced discrete random variables: For a discrete random variable, it makes sense to ask \\(P(Y = r)\\) for some \\(r\\). I.e. there will be some \\(r\\) such that this number is non-zero. In fact we took this as our starting point for all of the discrete random distributions we have talked about. For a continuous random variable: \\(P(Y=x)\\) has us a little bit nervous. If \\(Y\\) is truly continuous then near \\(x\\) there are infinitely many (uncountably infinitely many) values that are also possible. They can’t all have a non-zero value without us having trouble adding them all up. First we need to make our definition precise: How will we recognize a continuous random variable? (if you were in my class last summer you might recall our sandwich activity). One picture that should have you thinking from earlier this week is the graphs we made of the Cummulative Distribution Function for our random variables, here is the one for the binomials random variable: n &lt;- 10 p &lt;- 0.38 r &lt;- c(-1:(50*n))/50 plot(r, pbinom(r, n, p), type=&quot;l&quot;) Consider instead what this graph will need to look like for a random variable that can take any value rather than just the integer values? Recall the properties of our CDF \\(F(x) = P(Y \\leq x)\\): \\(\\lim_{x\\to -\\infty} F(x) = 0\\) \\(0 \\leq F(x) \\leq 1\\) for all x. \\(F(x)\\) is a non decreasing function of \\(x\\): given \\(x_1 &lt; x_2\\) then \\(F(x_1) &lt; F(x_2)\\). \\(\\lim_{x\\to \\infty} F(x) = 1\\) A Continuous Random Variable is one for which the cumulative distribution function: \\(F(x) = P(Y \\leq x)\\) is a continuous function. Note that we can’t say a discrete random variable is one where the CDF is not-continuous as it could have continuous and non-continuous parts. There is a whole theory of decomposing a general random variable into discrete and continuous components that is beyond what we want to do for this class. The next thing to note is that for our discrete random variables, the jump for the steps in the CDF is the probability distribution for that value. For a continuous random variable then, the probability that the variable achieves any specific value must be 0 because otherwise $P(Y = x) $ would be the size of a jump discontinuity in the CDF. 5.0.0.1 Probability Density Function So we don’t have a distribution in the sense that we do for discrete random variables, however note that for continuous functions the idea which captures how much the function is increasing (the size of the steps) is given by the derivative. This leads to: The Probability Density Function of a continuous random variable with CDF \\(F(x) = P(Y \\leq x)\\) is given by: \\[ f(x) = \\frac{d}{dx} F(x) \\] where the derivative exists. Note a few consequences: $ f(x) $ $ _{-}^f(x) dx = 1$ 5.0.1 Why is this called the density function? Let’s assume that \\(f(x)\\) exists everywhere. Then the Fundamental Theorem of Calculus implies that: \\[ F(x) = P(Y \\leq x) = \\int_{-\\infty}^x f(t) dt \\] Suppose we wanted to know \\(P(a \\leq Y \\leq b)\\)? On the one hand we compute this from the CDF: \\[ P( a \\leq Y \\leq b) = F(b) - F(a) \\] However we can rewrite this in terms of the PDF using the algebra of integrals: \\[ P(a \\leq Y \\leq b) = \\int_a^b f(x) dx \\] I.e. the area under \\(f(x)\\) over a region of \\(x\\)-space gives the probability that \\(x\\) lies in that region. Note some consequences: This is another way of thinking about why $P(Y = x) = 0 $: It corresponds to an integral over a single point. I think of this as density in the same sense as we would use in Physics. The integral of the density function for an object gives the mass of that object. 5.1 Uniform Distribution For our first example consider the cummulative distribution function: \\[ F(x) = \\left\\{ \\begin{matrix} 0 &amp; x &lt; 0 \\\\ x &amp; 0 \\leq x \\leq 1 \\\\ 1 &amp; x &gt; 1 \\end{matrix} \\right\\} \\] To plot this, we first need to define the function. We will go over this in class, but this is the syntax for definition a function in R and also because it is piecewise the syntax for if statements. Note this is the most complicated programing we will do. A couple of notes about using this in R - from the notebook you need to execute this command with the cursor at the very top, working from the middle will only run the middle block of code. The other note is that I want to write the function so that it handles columns the same way the builtin functions we have already met do. That means I need to assume the inpute is a list of values and it needs to return a list of values. # We need to write functions so they take a column of input values and return a column of output values, # This will make what we do next easier. F &lt;- function(x) { result &lt;- c() for (k in x) { # The actual function - note the if statements we need because of the piecewise nature if (k &lt; 0) { result &lt;- c(result, 0) } else if (k &gt; 1) { result &lt;- c(result, 1) } else { result &lt;- c(result, k) } } result } # testing that it works F(c(-0.25, 0.25, 3)) ## [1] 0.00 0.25 1.00 With that defined we can now plot the function. n &lt;- 100 x &lt;- c((-2*n):(2*n))/n plot(x, F(x), type = &quot;l&quot;) This is our cumulative distribution. Without even computing anything we can tell that the support of our density is on the interval \\([0, 1]\\). Note that we have corners in the CDF so there are two places where the density is not defined - your instincts from Calculus are correct that not having a density at a discrete set of values is not a big deal. You may recall from MATH 534 that there are some particularly nasty functions, but while they are theoretically interesting as CDFs they do not correspond to random variables we see often. We can compute the probability density function by differentiating this one. We get a piecewise defined function: \\[ f(x) = \\left\\{ \\begin{matrix} 0 &amp; x &lt; 0 \\\\ 1 &amp; 0 \\leq x \\leq 1 \\\\ 0 &amp; x &gt; 1 \\end{matrix} \\right\\} \\] This is an example of a uniform continuous distribution. On the support of the variable, the probability that \\(Y\\) is in an interval is proportional to the length of that interval. 5.1.0.1 General Unifrom Distributions Find the constant \\(C\\) such that \\(f(x)\\) below is a valid PDF: \\[ f(x) = \\left\\{ \\begin{matrix} 0 &amp; x &lt; a \\\\ C &amp; a \\leq x \\leq b \\\\ 0 &amp; x &gt; b \\end{matrix} \\right\\} \\] "],["class-schedule-and-activities.html", "A Class Schedule and Activities", " A Class Schedule and Activities Monday; June 14 - Introduction, using R, and the Binomial Distribution: Number of Students in your class? Mirror or non-mirror video for Zoom? Odometer Readings. How many students did you have your classes this Spring? Before class, bring the odometer readings from all of the cars your family has. Making a frequency table of the digits from the readings. What is the most common digit, what is the least common digit? Are you surprised by what you got, can you explain why you have the distribution you have? Consider the poll: what can we conclude about the proportion of people that have a preference for mirrored videos in their Zoom casts? Tuesday; June 15 - Geometric and Poisson Distributions. Traffic Cameras: How many pickup trucks versus cars? How many vehicles per minute? How many people are getting in line at the supermarket? Find a traffic camera for a section of free way or an intersection. How many cars go through before the first pickup truck you see? What can we conclude about the proportion of pickup trucks? With your team, watch the video for one minute and count how many vehicles you see. Do this a couple of times. How unusual would it be to see twice as many vehicles or more in a minute? How unusual would it be to see half as many or fewer vehicles in a minute? How many vehicles would we expect to see in an hour? How likely would it be to see that many or fewer in that hour? Wednesday; June 16 - Moments and moment generating functions; Continuous distributions. Uniform and Normal Distributions. Odometer Readings? Gas Prices? Bring the odometer readings from all of the cars your family has. How likely is it that we have an odometer reading more than 100,000 miles? Use Gas Buddy or a similar website to find the lowest price of gas within 10 miles of your house (approximately). How likely is it that the cheapest gas near us is less than $2.90? Thursday; June 17 - Gamma, and Beta Distributions: Radioactive Decay? Where will a pedestrian cross a road? The radioactive decay problem from the motivating examples. Let’s consider a section of street bounded by two intersections with traffic lights. We want to model where a pedestrian will decide to cross the street. "],["r-notes.html", "B R Notes", " B R Notes Putt suggested code for doing tasks here. "]]
