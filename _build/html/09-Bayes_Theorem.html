
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Bayes Theorem &#8212; MATH 550 - Applied Probability and Statistics</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Class Schedule and Activities" href="99-Apendices.html" />
    <link rel="prev" title="Confidence Intervals and Hypothesis Testing" href="08-Confidence_Hypothesis.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      <h1 class="site-logo" id="site-title">MATH 550 - Applied Probability and Statistics</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01-Introduction.html">
   Introduction and Course Syllabus
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="02-What_is_Stats.html">
   What is Statistics?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-Discrete_Probability.html">
   Discrete Random Variables - The Binomial Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04-Geometric_and_Poisson.html">
   Geometric and Poisson Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05-Continuous.html">
   Continuous Random Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06-Gamma_and_beta.html">
   Gamma and Beta Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07-Sampling.html">
   Sampling from Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08-Confidence_Hypothesis.html">
   Confidence Intervals and Hypothesis Testing
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Bayes Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="99-Apendices.html">
   Class Schedule and Activities
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/09-Bayes_Theorem.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/09-Bayes_Theorem.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example">
   Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Bayes Theorem
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-monty-hall">
     Example - Monty Hall
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-the-locomotive-problem">
   Example - The Locomotive Problem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-unfair-coin">
   Example - Unfair Coin
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#more-data">
     More Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#swamping-the-prior">
     Swamping the Prior
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#credible-intervals">
   Credible Intervals
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#detecting-fraud">
   Detecting Fraud
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bayes-theorem">
<h1>Bayes Theorem<a class="headerlink" href="#bayes-theorem" title="Permalink to this headline">¶</a></h1>
<p>This chapter has been evolving into my favorite of the whole course. In truth, were it not that hypothesis tests and confidence intervals were still being widely used we could have used this topic as the foundation for the course. A graduate program in statistics would feature an entire course dedicated to this topic.</p>
<p>So what is Bayes Theorem. To get there we need to start with the conditional probability that we last saw in Chapter 7 in our discussion of multivariate distributions and dependence.</p>
<p>Recall the definition:</p>
<div class="math notranslate nohighlight">
\[ P(A | B) = \mbox{The probability that event A happens given that we have observed event B} \]</div>
<p>Recall that this can be computed from</p>
<div class="math notranslate nohighlight">
\[ P(A, B) = \mbox{The probability that both event A and event B happen} \]</div>
<p>and the marginal probability</p>
<div class="math notranslate nohighlight">
\[ P(B) = \mbox{The probability that event B happens} \]</div>
<p>We get:</p>
<div class="math notranslate nohighlight">
\[ P(A|B) = \frac{P(A, B)}{ P(B) } \]</div>
<p>i.e. the probability that <span class="math notranslate nohighlight">\(A\)</span> happens given that <span class="math notranslate nohighlight">\(B\)</span> has been observed is found from the ratio of how likely A is to occur in the set of all outcomes where <span class="math notranslate nohighlight">\(B\)</span> has occured.</p>
<p>The observation that leads us to Bayes’s Theorem is that the denominator of this ratio <span class="math notranslate nohighlight">\(P(A, B)\)</span> is symmetric in <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. Hence it appears when we compute the conditional probability in the other direction:</p>
<div class="math notranslate nohighlight">
\[ P(B|A) = \frac{P(A, B)}{ P(A) } \]</div>
<p>This implies first that we can compute one conditional probability by finding the opposite one:</p>
<div class="math notranslate nohighlight">
\[ P(A|B) = \frac{P(A) P(B | A)}{P(B)} \]</div>
<p>which immediately lets us do things like:</p>
<div class="section" id="example">
<h2>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h2>
<p>Suppose that we draw two cards and show them to a friend. The friend tells us that at least one of them is a face card. How likely is it that one of the cards is the Queen of Hearts?  We have</p>
<p>First the probability that drawing two cards one of them is the QH is computed by adding together the probabilities that the first card or the second card is the QH - it helps that there is only one QH.
$<span class="math notranslate nohighlight">\( P(QH) = \frac{1}{52} \cdot \frac{51}{51} + \frac{51}{52} \cdot \frac{1}{51} = \frac{2}{52} \)</span>$</p>
<p>The probability that drawing two cards at least one of them is a Face Card is computed by adding together the probabilities that the first card is a FC but the second is not; that the second is a FC but the first is not; and finally that both of them are FC:
$<span class="math notranslate nohighlight">\( P(FC) = \frac{12}{52} \frac{51 - 11}{52} + \frac{51-12}{52} \frac{12}{51} + \frac{12}{52} \frac{11}{51} \)</span>$</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">PFC</span> <span class="o">=</span> <span class="m">12</span><span class="o">/</span><span class="m">52</span> <span class="o">*</span> <span class="p">(</span><span class="m">51-11</span><span class="p">)</span><span class="o">/</span><span class="m">51</span> <span class="o">+</span> <span class="p">(</span><span class="m">51-12</span><span class="p">)</span><span class="o">/</span><span class="m">52</span><span class="o">*</span> <span class="m">12</span><span class="o">/</span><span class="m">51</span> <span class="o">+</span> <span class="m">12</span><span class="o">/</span><span class="m">52</span> <span class="o">*</span> <span class="m">11</span><span class="o">/</span><span class="m">51</span>
<span class="n">PFC</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.407239819004525</div></div>
</div>
<p>Finally we want the probability that at least one of the cards is a face card given that one of them is QH, but this is easily seen to be 1.</p>
<p>Therefore we have:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">PQH_given_FC</span> <span class="o">=</span> <span class="m">2</span><span class="o">/</span><span class="m">52</span> <span class="o">/</span> <span class="n">PFC</span>
<span class="n">PQH_given_FC</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.0944444444444444</div></div>
</div>
</div>
<div class="section" id="id1">
<h2>Bayes Theorem<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>So that is interesting, but that is only part of the story. The situation Bayes wanted to undeerstand is the following:  Suppose that we have some event we are trying to understand <span class="math notranslate nohighlight">\(B\)</span> and a set of alternative events <span class="math notranslate nohighlight">\(B_2\)</span> and <span class="math notranslate nohighlight">\(B_3\)</span> such that <span class="math notranslate nohighlight">\(P(B + B_2 + B_3) = 1\)</span> i.e. all three together are the complete space.</p>
<p>We make some observation <span class="math notranslate nohighlight">\(D\)</span> via an experiment or otherwise by gaining new information about the likliehood of <span class="math notranslate nohighlight">\(B\)</span> and its complements. Our question is then, can we update how likely we think <span class="math notranslate nohighlight">\(B\)</span> and its complements are?</p>
<p>Bayes Theorem says that we can provided we can understand how likely the observation <span class="math notranslate nohighlight">\(D\)</span> was given each of the events <span class="math notranslate nohighlight">\(B\)</span> and its complements.</p>
<div class="math notranslate nohighlight">
\[ P(B |D) = \frac{P(B) P(D|B)}{P(D)} \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(B)\)</span> is called our <em>prior estimate</em> it is built out of assumptions we are making or other knowledge we might have about the likliehood of <span class="math notranslate nohighlight">\(B\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B|D)\)</span> is called the <em>posterior estimate</em> it represents the update we make to our assumptions in the prior estimate based on the observation we have made.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(D)\)</span> is the total likliehood of observation <span class="math notranslate nohighlight">\(D\)</span> over all possible outcomes</p></li>
<li><p><span class="math notranslate nohighlight">\(P(D|B)\)</span> is the likliehood of observation <span class="math notranslate nohighlight">\(D\)</span> in the case when event <span class="math notranslate nohighlight">\(B\)</span> happens.</p></li>
</ul>
<p>Let’s use some examples to explore:</p>
<div class="section" id="example-monty-hall">
<h3>Example - Monty Hall<a class="headerlink" href="#example-monty-hall" title="Permalink to this headline">¶</a></h3>
<p>The classic example is the Monty Hall Problem. You are on a game show and are confronted with three doors. Behind one of the doors is a new car, and behind the other two doors are goats. The game works in the following way:  You choose one of the doors. Then Monty opens another door revealing a goat behind it. He then asks you:  Do you want to change your guess?</p>
<p>The question is what should you do?</p>
<p>We will make a small table showing the computation. We first need to set our hypothesis for the possible events. In this case we can say that the contestant chooses door A, Monty opens door B. The car is behind A, B, or C. Our initial guess, because we have no knowledge otherwise is that the car is equally likly to be behind each door so a probability of 1/3. The observation <span class="math notranslate nohighlight">\(D\)</span> is that there is a goat behind door <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p>The likliehod <span class="math notranslate nohighlight">\(P(D | H)\)</span> is then found by in the case that the car is actually behind door A there is a 1/2 chance Monty will open door B. However given that the contestant has choosen door A, if the car is actually behind door C then Monty must open door B.</p>
<p>The <span class="math notranslate nohighlight">\(P(D)\)</span> is found by summing up the 4th column (which I remember by recognizing it is the numerator of Bayes ratio). In this case we get 1/3 + 1/6 = 1/2</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>H</p></th>
<th class="head"><p>Prior P(H)</p></th>
<th class="head"><p>Likliehood P(D given H)</p></th>
<th class="head"><p>P(H) P(D given H)</p></th>
<th class="head"><p>Posterior P(H given D)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>A</p></td>
<td><p>1/3</p></td>
<td><p>1/2</p></td>
<td><p>1/6</p></td>
<td><p>1/3</p></td>
</tr>
<tr class="row-odd"><td><p>B</p></td>
<td><p>1/3</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>C</p></td>
<td><p>1/3</p></td>
<td><p>1</p></td>
<td><p>1/3</p></td>
<td><p>2/3</p></td>
</tr>
</tbody>
</table>
<p>The final check that is that the last column should sum to 1 if we have made no arithmetic mistakes.</p>
<p>In thise case if the contestant switches their guess they will double their odds of winning the car.</p>
</div>
</div>
<div class="section" id="example-the-locomotive-problem">
<h2>Example - The Locomotive Problem<a class="headerlink" href="#example-the-locomotive-problem" title="Permalink to this headline">¶</a></h2>
<p>A train company has <span class="math notranslate nohighlight">\(N\)</span> locomotives and numbers them 1, 2, 3, … N.  We are at the train station and observe locomotive 65 go by. What can we say about <span class="math notranslate nohighlight">\(N\)</span>?</p>
</div>
<div class="section" id="example-unfair-coin">
<h2>Example - Unfair Coin<a class="headerlink" href="#example-unfair-coin" title="Permalink to this headline">¶</a></h2>
<p>Unfair coins happen. You can buy them at a magic or toy shop if you are interested.</p>
<p>We suspect we are dealing with an unfair coin. We flip it once and we get a heads. What can we say about the probability <span class="math notranslate nohighlight">\(p\)</span> that a flip of the coin will give us heads?</p>
<p>This is our first example of Bayes Theorem with a continuous hypothesis. We will ask what is the probability density function for the probability <span class="math notranslate nohighlight">\(p\)</span> that the coin gives a heads. You might recall that this is the kind of question that led to the Beta Distribution.</p>
<p>So each value of <span class="math notranslate nohighlight">\(p \in [0, 1]\)</span> is a hypothesis. We need to give a prior estimate of the probability for each p, and because it is a continuous variable we should do this by assigning a PDF to it.</p>
<p><strong>NOTE</strong>  Before we go on, you could solve this problem by setting some discrete values for <span class="math notranslate nohighlight">\(p\)</span> and turning it into a problem like the discrete examples above.</p>
<p>A sensible prior, not knowing anything else about the coin would be a flat prior i.e. that <span class="math notranslate nohighlight">\(p\)</span> is a uniformly distributed variable on the interval <span class="math notranslate nohighlight">\([0,1]\)</span>. So we have:  <span class="math notranslate nohighlight">\(f(x) = 1\)</span> for our prior.</p>
<p>Given a value of <span class="math notranslate nohighlight">\(p=x\)</span>, the likliehood of flipping a heads is x and thus: <span class="math notranslate nohighlight">\(P(D | p=x) = x\)</span>.</p>
<p>We then have <span class="math notranslate nohighlight">\( f(x) P(D | p=x) = x\)</span> and the total probability of observing a Heads is found by taking the integral:</p>
<div class="math notranslate nohighlight">
\[ \int_0^1 x dx = \frac12 \]</div>
<p>Finally putting it all together we get our posterior estimate:</p>
<div class="math notranslate nohighlight">
\[ f(x|D) = 2 x \]</div>
<div class="section" id="more-data">
<h3>More Data<a class="headerlink" href="#more-data" title="Permalink to this headline">¶</a></h3>
<p>One of the brilliant things about Bayes Theorem is that allows us to continuously update our estimate of the probability of our hypothesis as more data arrives. Suppose we flip the coin again and get another heads:</p>
<div class="math notranslate nohighlight">
\[ f(x | HH) = \frac{f(x |H) P(HH | x)}{f(HH)} \]</div>
<p>We have <span class="math notranslate nohighlight">\(f(x|H) = 2 x\)</span></p>
<p>The likliehood of a second H given p=x is just x again. We compute the total probability by finding the integral</p>
<div class="math notranslate nohighlight">
\[ f(HH) = \int_0^1 2 x^2 dx = \frac{2}{3} \]</div>
<p>and then we have our new posterior:</p>
<p>$<span class="math notranslate nohighlight">\( f(x | HH) = 3 x^2 \)</span>$.</p>
<p>Checking the graphs of our prior and the two posterior sequence we get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">&lt;-</span> <span class="m">200</span>
<span class="n">x</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="o">:</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
<span class="n">v0</span> <span class="o">&lt;-</span> <span class="nf">dunif</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
<span class="n">v1</span> <span class="o">&lt;-</span> <span class="n">x</span><span class="o">/</span><span class="m">2</span>
<span class="n">v2</span> <span class="o">&lt;-</span> <span class="n">x^2</span><span class="o">/</span><span class="m">3</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v0</span><span class="p">,</span> <span class="n">type</span><span class="o">=</span><span class="s">&#39;l&#39;</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&#39;red&#39;</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span> <span class="m">1.1</span><span class="p">))</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&#39;green&#39;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v2</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&#39;blue&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-Bayes_Theorem_7_0.png" src="_images/09-Bayes_Theorem_7_0.png" />
</div>
</div>
</div>
<div class="section" id="swamping-the-prior">
<h3>Swamping the Prior<a class="headerlink" href="#swamping-the-prior" title="Permalink to this headline">¶</a></h3>
<p>A major concern about using Bayes Theorem is what the effect of the prior is. If the prior estimate is having a strong effect on the result, then mistakes we make in setting it could lead us to the wrong conclusions. In many problems what we will find is that as the amount of data we include, either through refinement of the posterior estimate or just in one go increases, the effect of the prior is lessened - using the different priors does not change the conclusions very much. This is called swamping the prior.</p>
<p>For example: We are trying to estimate support for candidate A in the local mayoral race. We go to a polling location and we ask people leaving who they voted for. We ask 4 people before finding the first person who said they voted for candidate A. What proportion of the population is supporting candidate <span class="math notranslate nohighlight">\(A\)</span>?</p>
<p>We start by setting up a discrete set of hypothesis fix some large but not too large <span class="math notranslate nohighlight">\(N\)</span>:</p>
<div class="math notranslate nohighlight">
\[H_n = \left\{  p = n/N \right\} \]</div>
<p>A flat prior gives every <span class="math notranslate nohighlight">\(H_n\)</span> equal probability:  <span class="math notranslate nohighlight">\(P(H_n) = 1/N\)</span>.</p>
<p>The likliehood, given <span class="math notranslate nohighlight">\(p = n/N\)</span> of having to ask 4 voters before our first success is given by</p>
<div class="math notranslate nohighlight">
\[P(D | H_n) = (1 - n/N)^3 \cdot n/N \]</div>
<p>Multiplying this by the prior gives:</p>
<div class="math notranslate nohighlight">
\[ P(H_n) P(D|H_n) = (1-n/N)^3 \cdot n/N^2 \]</div>
<p>and then the total probability of the observation is found by adding all of these up. At this point lets choose an <span class="math notranslate nohighlight">\(N\)</span> and do this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">&lt;-</span> <span class="m">100</span>
<span class="n">Hn</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">N</span><span class="p">)</span><span class="o">/</span><span class="n">N</span>
<span class="n">prior1</span> <span class="o">&lt;-</span> <span class="nf">ceiling</span><span class="p">(</span> <span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">N</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="p">)</span><span class="o">/</span><span class="n">N</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">Hn</span><span class="p">,</span> <span class="n">prior1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-Bayes_Theorem_9_0.png" src="_images/09-Bayes_Theorem_9_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">PD_given_Hn</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="m">1</span><span class="o">-</span> <span class="n">Hn</span><span class="p">)</span><span class="n">^3</span> <span class="o">*</span> <span class="n">Hn</span>
<span class="n">PD_given_Hn</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>0.00970299</li><li>0.01882384</li><li>0.02738019</li><li>0.03538944</li><li>0.04286875</li><li>0.04983504</li><li>0.05630499</li><li>0.06229504</li><li>0.06782139</li><li>0.0729</li><li>0.07754659</li><li>0.08177664</li><li>0.08560539</li><li>0.08904784</li><li>0.09211875</li><li>0.09483264</li><li>0.09720379</li><li>0.09924624</li><li>0.10097379</li><li>0.1024</li><li>0.10353819</li><li>0.10440144</li><li>0.10500259</li><li>0.10535424</li><li>0.10546875</li><li>0.10535824</li><li>0.10503459</li><li>0.10450944</li><li>0.10379419</li><li>0.1029</li><li>0.10183779</li><li>0.10061824</li><li>0.09925179</li><li>0.09774864</li><li>0.09611875</li><li>0.09437184</li><li>0.09251739</li><li>0.09056464</li><li>0.08852259</li><li>0.0864</li><li>0.08420539</li><li>0.08194704</li><li>0.07963299</li><li>0.07727104</li><li>0.07486875</li><li>0.07243344</li><li>0.06997219</li><li>0.06749184</li><li>0.06499899</li><li>0.0625</li><li>0.06000099</li><li>0.05750784</li><li>0.05502619</li><li>0.05256144</li><li>0.05011875</li><li>0.04770304</li><li>0.04531899</li><li>0.04297104</li><li>0.04066339</li><li>0.0384</li><li>0.03618459</li><li>0.03402064</li><li>0.03191139</li><li>0.02985984</li><li>0.02786875</li><li>0.02594064</li><li>0.02407779</li><li>0.02228224</li><li>0.02055579</li><li>0.0189</li><li>0.01731619</li><li>0.01580544</li><li>0.01436859</li><li>0.01300624</li><li>0.01171875</li><li>0.01050624</li><li>0.00936859</li><li>0.00830544</li><li>0.00731619</li><li>0.0064</li><li>0.00555579</li><li>0.00478224</li><li>0.00407779</li><li>0.00344064</li><li>0.00286875</li><li>0.00235984</li><li>0.00191139</li><li>0.00152064</li><li>0.00118459</li><li>0.000899999999999999</li><li>0.000663389999999999</li><li>0.000471039999999999</li><li>0.000318989999999999</li><li>0.000203040000000001</li><li>0.00011875</li><li>6.14400000000002e-05</li><li>2.61900000000001e-05</li><li>7.84000000000002e-06</li><li>9.90000000000003e-07</li><li>0</li></ol>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">PD</span> <span class="o">&lt;-</span> <span class="nf">sum</span><span class="p">(</span><span class="n">prior1</span> <span class="o">*</span> <span class="n">PD_given_Hn</span><span class="p">)</span>
<span class="n">PD</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.049991667</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">posterior1</span> <span class="o">&lt;-</span> <span class="n">prior1</span> <span class="o">*</span> <span class="n">PD_given_Hn</span> <span class="o">/</span> <span class="n">PD</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">Hn</span><span class="p">,</span> <span class="n">posterior1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-Bayes_Theorem_12_0.png" src="_images/09-Bayes_Theorem_12_0.png" />
</div>
</div>
<p>Whereas maybe we suspect that there is not much support for our candidate. We might take</p>
<div class="math notranslate nohighlight">
\[ P(H_n) = (1 - n/N) / C\]</div>
<p>where <span class="math notranslate nohighlight">\(C\)</span> is a normalizing constant we will need to compute.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">prior2</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="m">1</span> <span class="o">-</span> <span class="n">Hn</span><span class="p">)</span>
<span class="n">prior2</span> <span class="o">&lt;-</span> <span class="n">prior2</span> <span class="o">/</span> <span class="nf">sum</span><span class="p">(</span><span class="n">prior2</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">Hn</span><span class="p">,</span> <span class="n">prior2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-Bayes_Theorem_14_0.png" src="_images/09-Bayes_Theorem_14_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># check that the prior estimate has total probability 1</span>
<span class="nf">sum</span><span class="p">(</span><span class="n">prior2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">1</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">PD</span> <span class="o">&lt;-</span> <span class="nf">sum</span><span class="p">(</span><span class="n">prior2</span> <span class="o">*</span> <span class="n">PD_given_Hn</span><span class="p">)</span>
<span class="n">PD</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.0673232333333333</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">posterior2</span> <span class="o">=</span> <span class="n">prior2</span> <span class="o">*</span> <span class="n">PD_given_Hn</span> <span class="o">/</span> <span class="n">PD</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">Hn</span><span class="p">,</span> <span class="n">posterior2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-Bayes_Theorem_17_0.png" src="_images/09-Bayes_Theorem_17_0.png" />
</div>
</div>
<p>We see that with a sample size of just 4 (essentially the 3 failures and then one success counts as 4 samples from the Bernouli trial) the prior we start with has little effect on the result.</p>
</div>
</div>
<div class="section" id="credible-intervals">
<h2>Credible Intervals<a class="headerlink" href="#credible-intervals" title="Permalink to this headline">¶</a></h2>
<p>Let’s use the last example we have computed to do something like we did with <em>confidence intervals</em>.  If we add up a set of hypothesis we get a range of values where we now have an estimate on how likely it is that the paramter lives. In the example above we note that adding up the probability for the first 51 values of p we get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">sum</span><span class="p">(</span><span class="n">posterior2[0</span><span class="o">:</span><span class="m">51</span><span class="n">]</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.904061786165615</div></div>
</div>
<p>With probability 90% then we are estimating that <span class="math notranslate nohighlight">\(p\)</span> is beetween 0 and 0.51. Note that the first posterior estimate gives a close result though not exactly the same:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">sum</span><span class="p">(</span><span class="n">posterior1[0</span><span class="o">:</span><span class="m">51</span><span class="n">]</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.830680309180328</div></div>
</div>
<p>Suppose we have even more data:  We poll another 3 people before finding a voter who supports A. We can update both of our posteriors again:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">PD_given_Hn</span> <span class="o">=</span> <span class="p">(</span><span class="m">1</span><span class="o">-</span> <span class="n">Hn</span><span class="p">)</span><span class="n">^2</span> <span class="o">*</span> <span class="n">Hn</span>
<span class="n">PD</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span> <span class="n">posterior1</span> <span class="o">*</span> <span class="n">PD_given_Hn</span><span class="p">)</span>
<span class="n">PD</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.119067454496966</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">posterior1_update</span> <span class="o">=</span> <span class="n">posterior1</span> <span class="o">*</span> <span class="n">PD_given_Hn</span> <span class="o">/</span> <span class="n">PD</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">Hn</span><span class="p">,</span> <span class="n">posterior1_update</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-Bayes_Theorem_24_0.png" src="_images/09-Bayes_Theorem_24_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">PD</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span> <span class="n">posterior2</span> <span class="o">*</span> <span class="n">PD_given_Hn</span><span class="p">)</span>
<span class="n">PD</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.119077371606741</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">posterior2_update</span> <span class="o">=</span> <span class="n">posterior2</span> <span class="o">*</span> <span class="n">PD_given_Hn</span> <span class="o">/</span> <span class="n">PD</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">Hn</span><span class="p">,</span> <span class="n">posterior2_update</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/09-Bayes_Theorem_26_0.png" src="_images/09-Bayes_Theorem_26_0.png" />
</div>
</div>
<p>and again looking for credible intervals we add up the first 49 values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">sum</span><span class="p">(</span> <span class="n">posterior1_update[1</span><span class="o">:</span><span class="m">49</span><span class="n">]</span><span class="p">);</span>
<span class="nf">sum</span><span class="p">(</span> <span class="n">posterior2_update[1</span><span class="o">:</span><span class="m">49</span><span class="n">]</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.84884061004525</div><div class="output text_html">0.905168735428658</div></div>
</div>
<p>As we make more observations we expect the updated posteriors to converge and therefore the credible intervals will continue to get closer. The evidence so far is indicating that candidate A has less than 50% support from voters.</p>
</div>
<div class="section" id="detecting-fraud">
<h2>Detecting Fraud<a class="headerlink" href="#detecting-fraud" title="Permalink to this headline">¶</a></h2>
<p>Bayes theorem is frequently used in efforts to detect fraud or determine with an email is legitimate or might be an attach on a company. One example of such an effort is in looking at the results of laboratory tests for THC concentration in commercial marijuana. It appears that some labs have an incentive of some kind to rate samples at above 20%. See the two histograms - one is the resutl of tests from a lab that has been suspended and the other is the result from a trusted laboratory.</p>
<p>This problem comes from the article: <a class="reference external" href="https://jcannabisresearch.biomedcentral.com/articles/10.1186/s42238-021-00064-2">https://jcannabisresearch.biomedcentral.com/articles/10.1186/s42238-021-00064-2</a> about cannabis testing labs in Washington state. There have been some high profile cases of labs in CA, NV, and CO as well as Canada being fine or suspended for falsifying data or for retailers or producers shopping for labs.</p>
<p>Something suspicious is happening at some labs in Washington state that test for THC concentrations in consumer sold marajuana. The distribution of concentrations appear to be normal except that there is a discontinuity in the distribution at 20% with a higher density of results just above 20% than just below 20%. Here is the histogram that was found:</p>
<img src="Labs.png" alt="Histogram of THC Concentrations in suspended Washington Labs" width="300"/>
<p>Whereas here is a distribution of test results from the largest lab in the sate:</p>
<img src="goodLab.png" alt="Histogram of THC Concentrations in unsuspended Washington Labs" width="300"/>
<p>Suppose that you have a set of results from a lab for concentrations of some samples. How would you go about deciding if they have manipulated the results?</p>
<p>This is an interesting question and it is one I would consider having a group of students spend an entire semester on. Also the idea of using statistics to detect people lying is a worthwhile approach to the course.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            kernelName: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="08-Confidence_Hypothesis.html" title="previous page">Confidence Intervals and Hypothesis Testing</a>
    <a class='right-next' id="next-link" href="99-Apendices.html" title="next page">Class Schedule and Activities</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>