
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Sampling from Distributions &#8212; MATH 550 - Applied Probability and Statistics</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Class Schedule and Activities" href="99-Apendices.html" />
    <link rel="prev" title="Gamma and Beta Distributions" href="06-Gamma_and_beta.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      <h1 class="site-logo" id="site-title">MATH 550 - Applied Probability and Statistics</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01-Introduction.html">
   Introduction and Course Syllabus
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="02-What_is_Stats.html">
   What is Statistics?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-Discrete_Probability.html">
   Discrete Random Variables - The Binomial Distribution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04-Geometric_and_Poisson.html">
   Geometric and Poisson Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05-Continuous.html">
   Continuous Random Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06-Gamma_and_beta.html">
   Gamma and Beta Distributions
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Sampling from Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="99-Apendices.html">
   Class Schedule and Activities
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/07-Sampling.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/07-Sampling.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling">
   Sampling
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multivariate-distributions">
   Multivariate Distributions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#marginal-probability">
     Marginal Probability
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-probability">
     Conditional Probability
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#idependent-random-variables">
     Idependent Random Variables
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example">
       Example
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-2">
       Example 2
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#correlation-and-covariance">
   Correlation and Covariance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algebra">
     Algebra
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-combinations-of-independent-random-variables">
     Linear Combinations of Independent Random Variables
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Example
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-sampling">
   Random Sampling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mean-of-the-sample">
     Mean of the sample
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id3">
       Example
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sample-from-a-normal-distribution">
       Sample from a Normal Distribution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-result">
     General Result
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#central-limit-theorem">
   Central Limit Theorem
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#moment-convergence-theorem">
     Moment Convergence Theorem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     Central Limit Theorem
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#proof">
       Proof
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#when-does-clt-not-apply">
       When does CLT not apply?
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-clt">
   Using CLT
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-do-we-mean-by-95-confident">
     What do we mean by 95% confident
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#some-questions">
     Some questions
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="sampling-from-distributions">
<h1>Sampling from Distributions<a class="headerlink" href="#sampling-from-distributions" title="Permalink to this headline">¶</a></h1>
<p>Let us start by recapping where we are:  We have met both generic and some particular <em>named</em> distributions that model situations, and in the case of <em>named</em> distributions that have sufficient structure or symmetry that we can compute with them.</p>
<p>A couple of problems then confront us:  In reality we rarely know the distribution or we may know the <em>type</em> of distribution but not the exact parameters for a random variable, and our only way to gather information about the distribution is to sample from it.</p>
<p>An analogy:  We are trying to identify a movie. Maybe we know it is a spy thriller. But otherwise all we have are some still images from it. How many images would we need to identify the movie?</p>
<p>We will take class to sample from our <em>named</em> distributions and see what happens. I’ll give one example here to illustrate the method:  Consider a shopping center where on average 2 customers arrive at the chashiers every minute. The number of customers <span class="math notranslate nohighlight">\(Y\)</span> that will arrive at the cashiers in a minute is then a random variable given by the Poisson distribution:</p>
<div class="math notranslate nohighlight">
\[ P(Y=x) = \frac{2^r}{r!} e^{-2} \]</div>
<p>Suppose we measure the number of customers arriving each minute for 5 minutes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sample</span> <span class="o">&lt;-</span> <span class="nf">rpois</span><span class="p">(</span><span class="m">5</span><span class="p">,</span> <span class="m">2</span><span class="p">)</span>
<span class="n">sample</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>2</li><li>3</li><li>2</li><li>2</li><li>3</li></ol>
</div></div>
</div>
<p>Some questions:</p>
<ul class="simple">
<li><p>What is the mean number of customers in our sample?</p></li>
<li><p>What do we get as we increase the size of the sample?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sample</span> <span class="o">&lt;-</span> <span class="nf">rpois</span><span class="p">(</span><span class="m">200</span><span class="p">,</span> <span class="m">2</span><span class="p">)</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">1.92</div></div>
</div>
<p>The histogram of our sample from the Poisson Distribution, looks more like the Poisson Distribution itself as the sample size increases.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">hist</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="m">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/07-Sampling_5_0.png" src="_images/07-Sampling_5_0.png" />
</div>
</div>
<p>Suppose we were to treat the mean of the sample <span class="math notranslate nohighlight">\(\bar{Y}\)</span> as a random variable itself. Fix the size of the sample and see what we get if we run the experiment many times, what shape do you see in the histogram for values of <span class="math notranslate nohighlight">\(\bar{Y}\)</span> and what happens to that shape as the size of the sample is then increased?</p>
<p>Before you run the experiment, generate a hypothesis. What will we get as we run the experiment more?  What will we get as we increase the size of the sample? Then as you run the experiment revise your hypothesis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">()</span>
<span class="nf">for </span><span class="p">(</span><span class="n">k</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="m">50</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">result</span> <span class="o">&lt;-</span> <span class="nf">c</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="nf">mean</span><span class="p">(</span><span class="nf">rpois</span><span class="p">(</span><span class="m">5</span><span class="p">,</span> <span class="m">2</span><span class="p">))</span> <span class="p">)</span>
<span class="p">}</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/07-Sampling_7_0.png" src="_images/07-Sampling_7_0.png" />
</div>
</div>
<div class="section" id="sampling">
<h2>Sampling<a class="headerlink" href="#sampling" title="Permalink to this headline">¶</a></h2>
<p>Run the experiment above with a variety of our named distributions including the normal distribution. What is happening?  Can you develop a hypothesis?</p>
</div>
<div class="section" id="multivariate-distributions">
<h2>Multivariate Distributions<a class="headerlink" href="#multivariate-distributions" title="Permalink to this headline">¶</a></h2>
<p>In an effort to make our hypothesis we have develop precise, we need to build up some langauge for modeling this situation.</p>
<p>A <em>multivariate distribution</em> is defined to be a distribution on multiple random variables describing the likliehood that values of them will come together. Explicitly given a set of discrete random variables <span class="math notranslate nohighlight">\(Y_1, Y_2, \dots, Y_n\)</span> we can define their joint probability distribution by:  <span class="math notranslate nohighlight">\(P(Y_1 = x_1, Y_2 = x_2, \dots, Y_n = x_n) \)</span> to be the probability that each of the <span class="math notranslate nohighlight">\(Y\)</span>s has the given value.</p>
<p>In the continuous case, we need to proceed as we did before:  We define the joint cummulative distribution by:</p>
<div class="math notranslate nohighlight">
\[ F(x_1, x_2, \dots, x_n) = P( Y_1 \leq x_1, Y_2 \leq x_2, \dots, Y_n \leq x_n) \]</div>
<p>and then the joint probability density function is given by:</p>
<div class="math notranslate nohighlight">
\[ f(x_1, x_2, \dots, x_n) = \frac{\partial^n F}{\partial x_1 \partial x_2 \dots \partial x_n}\]</div>
<p>Again do not fall for the trap of thinking about the density as the distribution:  the values of <span class="math notranslate nohighlight">\(f\)</span> do not give the likliehood of a particular outcome.</p>
<div class="section" id="marginal-probability">
<h3>Marginal Probability<a class="headerlink" href="#marginal-probability" title="Permalink to this headline">¶</a></h3>
<p>Given two discrete random variables <span class="math notranslate nohighlight">\(Y_1\)</span> and <span class="math notranslate nohighlight">\(Y_2\)</span>: we define the marginal probability of <span class="math notranslate nohighlight">\(Y_1\)</span> to be the total likliehood that <span class="math notranslate nohighlight">\(Y_1\)</span> occurs.</p>
<div class="math notranslate nohighlight">
\[ P(Y_1 = x_1) = \sum_{x_2} P(Y_1 = x_1, Y_2 = x_2) \]</div>
<p>In the continuous case the marginal PDF for <span class="math notranslate nohighlight">\(Y_1\)</span> is found by:</p>
<div class="math notranslate nohighlight">
\[ f_1(x_1) = \int f(x_1, x_2) dx_2 \]</div>
</div>
<div class="section" id="conditional-probability">
<h3>Conditional Probability<a class="headerlink" href="#conditional-probability" title="Permalink to this headline">¶</a></h3>
<p>Consider two discrete random variables: We define the conditional probability <span class="math notranslate nohighlight">\(P(Y_1=x_1 | Y_2 = x_2)\)</span>, the probability that <span class="math notranslate nohighlight">\(Y_1 = x_1\)</span> given that we have observed that <span class="math notranslate nohighlight">\(Y_2 = x_2\)</span>. This can be compute by recognizing that the likliehood of <span class="math notranslate nohighlight">\(Y_1\)</span> occuring given a value for <span class="math notranslate nohighlight">\(Y_2\)</span> is the likliehood that both values occured randomly divided by the marginal probablity of <span class="math notranslate nohighlight">\(Y_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[ P(Y_1 = x_1 | Y_2 = x_2) = \frac{P(Y_1 =x_1, Y_2 = x_2)}{P(Y_2 = x_2)}\]</div>
<p>In other words the conditional probability given <span class="math notranslate nohighlight">\(Y_2=x_2\)</span> is the proportion of the time that <span class="math notranslate nohighlight">\(Y_1=x_1\)</span> occured out of all cases where <span class="math notranslate nohighlight">\(Y_2 = x_2\)</span> occured.</p>
<p>In the continuous case, the conditional PDF is found by:</p>
<div class="math notranslate nohighlight">
\[ f(x_1 | x_2) = \frac{f(x_1, x_2)}{ f_2(x_2) } \]</div>
</div>
<div class="section" id="idependent-random-variables">
<h3>Idependent Random Variables<a class="headerlink" href="#idependent-random-variables" title="Permalink to this headline">¶</a></h3>
<p>Consider the case of two discrete random varialbes with distribution <span class="math notranslate nohighlight">\(P(Y_1 = x_1, Y_2 = x_2)\)</span>. The two variables are independent if the likliehood of them both occuring is just the product of each one indvididually occuring (the marginal probabilities of each):</p>
<div class="math notranslate nohighlight">
\[ P(Y_1 = x_1, Y_2 = x_2) = P(Y_1 = x_1) P(Y_2 = x_2) \]</div>
<p>equivalently what we are saying is that the variables are independent if conditioning by one of them just gives the marginal likliehood:</p>
<div class="math notranslate nohighlight">
\[ P(Y_1 = x_1 | Y_2 = x_2) = P(Y_1 = x_1) \]</div>
<p>In the continuous case, indepdent random variables have a joint PDF that is just a product of marginal PDFs:</p>
<div class="math notranslate nohighlight">
\[ f(x_1, x_2) = f_1(x_1) f_2(x_2) \]</div>
<p>or in terms of the conditional PDF we will have:</p>
<div class="math notranslate nohighlight">
\[ f(x_1 | x_2) = f_1(x_1) \]</div>
<p>Again the idea is that if the variables are independent, knowing the value we got for one of them should not change our probability density of the other one.</p>
<div class="section" id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h4>
<p>Find the constant <span class="math notranslate nohighlight">\(c\)</span> such that</p>
<div class="math notranslate nohighlight">
\[\begin{split} f(x_1, x_2) = \left\{ \begin{matrix} C x_1 x_2 &amp; 0 \leq x_1, x_2 \leq 1 \\ 0 &amp; \mbox{otherwise} \end{matrix} \right. \end{split}\]</div>
<p>is a valid PDF. Are <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> indepdent?</p>
</div>
<div class="section" id="example-2">
<h4>Example 2<a class="headerlink" href="#example-2" title="Permalink to this headline">¶</a></h4>
<p>Find the constant <span class="math notranslate nohighlight">\(c\)</span> such that</p>
<div class="math notranslate nohighlight">
\[\begin{split} f(x_1, x_2) = \left\{ \begin{matrix} C x_1 x_2 &amp; 0 \leq x_1 &lt; x_2 \leq 1 \\ 0 &amp; \mbox{otherwise} \end{matrix} \right. \end{split}\]</div>
<p>is a valid PDF. Are <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> indepdent?</p>
<p>Reminder:  You can use wolfram alpha to compute the integral, but you may need to think about how to set it up correctly. You could determine independence/dependence without actually doing the integral if you appeal to the meaning of the definitions above.</p>
</div>
</div>
</div>
<div class="section" id="correlation-and-covariance">
<h2>Correlation and Covariance<a class="headerlink" href="#correlation-and-covariance" title="Permalink to this headline">¶</a></h2>
<p>Given a pair of jointly distributed continuous random variables <span class="math notranslate nohighlight">\(Y_1\)</span> and <span class="math notranslate nohighlight">\(Y_2\)</span> one measure of their dependence is to ask how far they jointly move from the means. Let</p>
<div class="math notranslate nohighlight">
\[ \mu_1 = E(Y_1) \qquad \mbox{and} \qquad \mu_2 = E(Y_2) \]</div>
<p>we have the variances</p>
<div class="math notranslate nohighlight">
\[ \sigma_1^2 = E( (Y_1 - \mu_1)^2) \qquad \mbox{and} \qquad \sigma_2^2 = E( (Y_2 - \mu_2)^2 ) \]</div>
<p>which again measure the extent to which <span class="math notranslate nohighlight">\(Y_1\)</span> (resp. <span class="math notranslate nohighlight">\(Y_2\)</span>) is likely to be far from its mean.</p>
<p>However, it is also interesting to ask how far <span class="math notranslate nohighlight">\(Y_1\)</span> will stray from its mean while <span class="math notranslate nohighlight">\(Y_2\)</span> is simmultaneously measured from its mean. We defin the covariance of <span class="math notranslate nohighlight">\(Y_1\)</span> and <span class="math notranslate nohighlight">\(Y_2\)</span> to be</p>
<div class="math notranslate nohighlight">
\[ \mbox{Cov}(Y_1, Y_2) = E( (Y_1 - \mu_1) (Y_2 - \mu_2) ) \]</div>
<p>Note that if <span class="math notranslate nohighlight">\(Y_1\)</span> and <span class="math notranslate nohighlight">\(Y_2\)</span> are independent then</p>
<div class="math notranslate nohighlight">
\[ \mbox{Cov}(Y_1, Y_2) = E( (Y_1 -\mu_1) ) E( (Y_2 - \mu_2) ) = 0 \cdot 0 = 0 \]</div>
<p>The units of <span class="math notranslate nohighlight">\(\mbox{Cov}(Y_1, Y_2)\)</span> are the products of the two units. Which means that this is not an absolute measure of the correlations between two varaibles; for example if one of the variables has a large variance it will mean a larger covariance even if the correlation is weak. We can get something that is unitless by dividing this by the square root of the product of variances. The <em>coefficient of correlation</em> is given by</p>
<div class="math notranslate nohighlight">
\[ \rho = \frac{\mbox{Cov}(Y_1, Y_2)}{\sqrt{ V(Y_1) V(Y_2) }}  \]</div>
<div class="section" id="algebra">
<h3>Algebra<a class="headerlink" href="#algebra" title="Permalink to this headline">¶</a></h3>
<p>Some algebra of the expected values gives:</p>
<div class="math notranslate nohighlight">
\[ \mbox{Cov}(Y_1, Y_2) = E( Y_1 Y_2) - E(Y_1) E(Y_2) \]</div>
<p>and so one way to think about covariance is that it is measuring how far the expected value of the product varies from what it would be if the variables were not correlated.</p>
</div>
<div class="section" id="id1">
<h3>Example<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Be careful no correlation does not imply independence. Consider the joint density:</p>
<div class="math notranslate nohighlight">
\[\begin{split} f(x_1, x_2) = \left\{ \begin{matrix} C &amp; -1 &lt; x_1 &lt; 0; \qquad 0 &lt; x_2 &lt; 1 + x_1 \\ C &amp; 0 &lt; x_1 &lt; 1; \qquad 0 &lt; x_2 &lt; 1-x_1 \\ 0 &amp; \mbox{otherwise} \end{matrix} \right. \end{split}\]</div>
<p>Show that for two random variables <span class="math notranslate nohighlight">\(Y_1\)</span> and <span class="math notranslate nohighlight">\(Y_2\)</span> with this join distribution <span class="math notranslate nohighlight">\(\mbox{Cov}(Y_1, Y_2) = 0\)</span> but that <span class="math notranslate nohighlight">\(Y_1\)</span> and <span class="math notranslate nohighlight">\(Y_2\)</span> are dependent.</p>
</div>
<div class="section" id="linear-combinations-of-independent-random-variables">
<h3>Linear Combinations of Independent Random Variables<a class="headerlink" href="#linear-combinations-of-independent-random-variables" title="Permalink to this headline">¶</a></h3>
<p>Suppose that <span class="math notranslate nohighlight">\(Y_1, \dots, Y_n\)</span> are independent random variables with <span class="math notranslate nohighlight">\(E(Y_i) = \mu_i\)</span> and <span class="math notranslate nohighlight">\(V(Y_i) = \sigma_i\)</span>. Let <span class="math notranslate nohighlight">\(U = \sum a_i Y_i \)</span> and <span class="math notranslate nohighlight">\(V = \sum b_i Y_i\)</span> for some coefficients <span class="math notranslate nohighlight">\(a_i\)</span> and <span class="math notranslate nohighlight">\(b_i\)</span>. What can you say about</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( E(U)\)</span> and <span class="math notranslate nohighlight">\(E(V)\)</span>?</p></li>
<li><p><span class="math notranslate nohighlight">\( V(U)\)</span> and <span class="math notranslate nohighlight">\(V(V)\)</span>?</p></li>
<li><p><span class="math notranslate nohighlight">\(\mbox{Cov}(U, V)\)</span>?</p></li>
<li><p>when will <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> have no correlation?</p></li>
<li><p>when will <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> be independent?</p></li>
<li><p>when will <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> have no correlation but be dependent?</p></li>
</ul>
<div class="section" id="id2">
<h4>Example<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<p>Supppose <span class="math notranslate nohighlight">\(Y\)</span> is a normal random variable with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Show that</p>
<div class="math notranslate nohighlight">
\[ Z = \frac{Y - \mu}{\sigma} \]</div>
<p>is a normal random variable with mean 0 and variance 1.</p>
</div>
</div>
</div>
<div class="section" id="random-sampling">
<h2>Random Sampling<a class="headerlink" href="#random-sampling" title="Permalink to this headline">¶</a></h2>
<p>This then brings us back to our sampling question. We can think of a random sample from a distribution, <span class="math notranslate nohighlight">\(Y_1, Y_2, \dots, Y_n\)</span> as a multivariate distribution of independent identically distributed random variables.</p>
<p>If the <span class="math notranslate nohighlight">\(Y_i\)</span> are discrete with distribution <span class="math notranslate nohighlight">\(P(Y = x) = p(x)\)</span> then:</p>
<div class="math notranslate nohighlight">
\[ P(Y_1 = x_1, Y_2 = x_2, \dots, Y_n = x_n) = p(x_1) p(x_2) \dots p(x_n) \]</div>
<p>If the <span class="math notranslate nohighlight">\(Y_i\)</span> are continuous with PDF <span class="math notranslate nohighlight">\(f(x)\)</span> then:</p>
<div class="math notranslate nohighlight">
\[ f(x_1, x_2, \dots, x_n) = f(x_1) f(x_2) \dots f(x_n)\]</div>
<p>We can then answer questions like:</p>
<div class="section" id="mean-of-the-sample">
<h3>Mean of the sample<a class="headerlink" href="#mean-of-the-sample" title="Permalink to this headline">¶</a></h3>
<p>The mean of the sample is given by:</p>
<div class="math notranslate nohighlight">
\[ \bar{Y} = \frac{1}{n} Y_1 + \frac{1}{n} Y_2 + \dots + \frac{1}{n} Y_n \]</div>
<p>Suppose we are in the continuous case, and we want to know what the distribution of <span class="math notranslate nohighlight">\(\bar{Y}\)</span> is?</p>
<p>Well its the same trick we have used over-and-over, we compute the CDF and then differentiate:</p>
<div class="math notranslate nohighlight">
\[ F(x) = P( \bar{Y} &lt; x ) = \iint \dots \int_{x_1 + x_2 + \dots + x_n \leq n x} f(x_1) f(x_2) \dots f(x_n) dx_1 dx_2 \dots dx_n \]</div>
<p>Okay so there is not much to do unless we have an explicit value for the PDF, and even then it is likely to get hairy quickly. However two points (a) it is exaclty a problem that a symbolic computer system is good at, and (b) it should not be surprising that we can compute with this.</p>
<p>We will do one example:</p>
<div class="section" id="id3">
<h4>Example<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<p>Using the linear combination result from above what can you say about <span class="math notranslate nohighlight">\(E(\bar{Y})\)</span> and <span class="math notranslate nohighlight">\(V(\bar{Y})\)</span>?</p>
<p>We will call <span class="math notranslate nohighlight">\(\bar{Y}\)</span> a point estimator for <span class="math notranslate nohighlight">\(\mu = E(Y)\)</span>.</p>
</div>
<div class="section" id="sample-from-a-normal-distribution">
<h4>Sample from a Normal Distribution<a class="headerlink" href="#sample-from-a-normal-distribution" title="Permalink to this headline">¶</a></h4>
<p>Let <span class="math notranslate nohighlight">\(Y_1\)</span> and <span class="math notranslate nohighlight">\(Y_2\)</span> be random samples from the Standard Normal distribution ( the normal distribution with mean 0 and variance 1).</p>
<p>The PDF for each is:</p>
<div class="math notranslate nohighlight">
\[ f(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \]</div>
<p>If the mean of the sample is then <span class="math notranslate nohighlight">\(\bar{Y} = \frac12 Y_1 + \frac12 Y_2 \)</span> then the CDF for the mean is:</p>
<div class="math notranslate nohighlight">
\[ F(x) = \iint_{x_1 + x_2 &lt; 2 x} \frac{1}{2\pi} e^{-x_1^2/2 - x_2^2/2} dx_1 dx_2 \]</div>
<p>We can set up this itereated integral by fixing an <span class="math notranslate nohighlight">\(x_1\)</span> and then integrating over all <span class="math notranslate nohighlight">\(x_2\)</span> values such that the inequality holds, and then integrating over all possible <span class="math notranslate nohighlight">\(x_1\)</span> values:</p>
<div class="math notranslate nohighlight">
\[ F(x) = \frac{1}{2\pi} \int_{-\infty}^\infty dx_1 e^{-x_1^2/2} \int_{-\infty}^{2x - x_1} dx_2 e^{-x_2^2/2} \]</div>
<p>Of course we don’t actually care what this is, because what we want to do is differentiate it to get the PDF. Notice that the <span class="math notranslate nohighlight">\(x\)</span> dependence is confined to just one bound on the iterated integral. We get:</p>
<div class="math notranslate nohighlight">
\[ f(x) = \frac{dF}{dx} = \frac{1}{\pi} \int_{-\infty}^\infty dx_1 e^{-x_1^2/2} e^{-(2x - x_1)^2/2} = \frac{1}{\pi} \int_{-\infty}^\infty dx_1 e^{-2x^2 + 2x x_1 - x_1^2} \]</div>
<p>If we complete the square in the exponent we get:</p>
<div class="math notranslate nohighlight">
\[ - ( x_1^2 - 2 x x_1) - 2 x^2 = - ( x_1 -x)^2 - x^2 \]</div>
<p>Then the integral is just the normalizing constant for a normal random variable with variance <span class="math notranslate nohighlight">\(\sigma^2 = 1/2\)</span> and mean <span class="math notranslate nohighlight">\(\mu = x\)</span> and so it gives <span class="math notranslate nohighlight">\(\sqrt{\pi}\)</span>. Therefore we have</p>
<div class="math notranslate nohighlight">
\[ f(x) = \frac{1}{\sqrt{pi}} e^{- x^2} \]</div>
<p>Which is the PDF for a normal distribution with mean 0 and a variance of <span class="math notranslate nohighlight">\(1/2\)</span>.</p>
</div>
</div>
<div class="section" id="general-result">
<h3>General Result<a class="headerlink" href="#general-result" title="Permalink to this headline">¶</a></h3>
<p>It’s not surprising then that the above result generalizes to a sample of size <span class="math notranslate nohighlight">\(n\)</span>:  Let <span class="math notranslate nohighlight">\(Y_1, Y_2, \dots, Y_n\)</span> be a sample from a normal distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Then the sample mean</p>
<div class="math notranslate nohighlight">
\[ \bar{Y} = \frac1n Y_1 + \frac1n Y_2 + \dots + \frac1n Y_n \]</div>
<p>is a random variable given by a normal distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and <strong>variance</strong> <span class="math notranslate nohighlight">\(\sigma^2 / n\)</span>.</p>
<p>In particular as we increase the size of the sample, the variance decreases as <span class="math notranslate nohighlight">\(1/n\)</span>.</p>
<p>The proof is just setting up the multi integral like we did above and then differentiating it, it is interesting, but not terribly enlightening so we will skip it.</p>
</div>
</div>
<div class="section" id="central-limit-theorem">
<h2>Central Limit Theorem<a class="headerlink" href="#central-limit-theorem" title="Permalink to this headline">¶</a></h2>
<p>So let’s return to our hypothesis. It was looking like, at least in the cases we were experimenting with that things became more normal as the size of the sample increased. It is one of the highlights of this subject that this is in fact a theorem.</p>
<div class="section" id="moment-convergence-theorem">
<h3>Moment Convergence Theorem<a class="headerlink" href="#moment-convergence-theorem" title="Permalink to this headline">¶</a></h3>
<p>First we need a workhorse theorem about the relationship between moment generating functions converging and the random variables they describe:</p>
<p>Let <span class="math notranslate nohighlight">\(Y, Y_1, Y_2, \dots, \)</span> be random variables with moment generating functions</p>
<div class="math notranslate nohighlight">
\[ m_j(t) = E( e^{ t Y_j}) \]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[ m(t) = E( e^{t Y} ) \]</div>
<p>Then if the moment generating functions converge to <span class="math notranslate nohighlight">\(m\)</span>:  $<span class="math notranslate nohighlight">\( \lim_{n\to \infty} m_n(t) = m(t) \)</span><span class="math notranslate nohighlight">\( then the distribution for \)</span>Y_n<span class="math notranslate nohighlight">\( converges to the distribution of \)</span>Y$.</p>
</div>
<div class="section" id="id4">
<h3>Central Limit Theorem<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>The central limit theorem is then that if <span class="math notranslate nohighlight">\(Y_1, Y_2, \dots, Y_n\)</span> are independent identiically distributed random variables with <span class="math notranslate nohighlight">\(E(Y_i) = \mu\)</span> and <span class="math notranslate nohighlight">\(V(Y_i) = \sigma^2 &lt; \infty\)</span> let</p>
<div class="math notranslate nohighlight">
\[ U_n = \frac{\bar{Y} - \mu}{\sigma/\sqrt{n}} \]</div>
<p>Then the distribution of <span class="math notranslate nohighlight">\(U_n\)</span> converges to the standard normal distribution as <span class="math notranslate nohighlight">\(n\to \infty\)</span>.</p>
<p>Explicitly:</p>
<div class="math notranslate nohighlight">
\[ \lim_{n\to \infty} P(U_n \leq u) = \int_{-\infty}^u \frac{1}{\sqrt{2\pi}} e^{-t^2/2} dt \]</div>
<p>Note how few assumptions we have on the distribution the <span class="math notranslate nohighlight">\(Y_i\)</span> are pulled from. They could even be discrete, hence the reason we have to phrase the last formula in terms of the CDF.</p>
<div class="section" id="proof">
<h4>Proof<a class="headerlink" href="#proof" title="Permalink to this headline">¶</a></h4>
<p>This is such a powerful result (it really feels to me like more than we deserve) and as such I want to give a proof of it here:</p>
<p>Define</p>
<div class="math notranslate nohighlight">
\[ Z_i = \frac{Y_i - \mu}{\sigma} \]</div>
<p>and then note that we have:</p>
<div class="math notranslate nohighlight">
\[ U_n = \frac{1}{\sqrt{n}} \sum_{i=1}^n Z_i \]</div>
<p>The <span class="math notranslate nohighlight">\(Z_i\)</span> are idependent identically distributed random variables with mean 0 and variance 1 (note that they are not necessarily normal unless the <span class="math notranslate nohighlight">\(Y\)</span> are normal). Since they are indepdent their moment generating functions multiply and are equal:</p>
<div class="math notranslate nohighlight">
\[ m_{\sum Z_i}(t) = m_{Z_1}(t) m_{Z_2}(t) \cdots m_{Z_n}(t) = m_Z(t)^n \]</div>
<p>Therefore</p>
<div class="math notranslate nohighlight">
\[ m_{U_n}(t) = m_Z(\frac{t}{\sqrt{n}})^n \]</div>
<p>We use Taylors Theorem from Calculus 2:</p>
<div class="math notranslate nohighlight">
\[ m_Z(t) = m_Z(0) + m'_Z(0) t + m''_Z(\xi) \frac{t^2}{2} \]</div>
<p>for some <span class="math notranslate nohighlight">\(0 &lt; \xi &lt; t \)</span>.</p>
<p>We have <span class="math notranslate nohighlight">\(m_Z(0) = E( e^{0 Z} ) = E( 1) = 1 \)</span> and <span class="math notranslate nohighlight">\(m'_Z(0) = E( Z) = 0 \)</span> and so</p>
<div class="math notranslate nohighlight">
\[ m_Z(t) =1 + m''_Z(\xi) \frac{t^2}{2} \]</div>
<p>Therefore:</p>
<div class="math notranslate nohighlight">
\[ m_{U_n}(t) = \left[ 1 + m''_Z(\xi_n) \frac{t^2}{2n} \right]^n \]</div>
<p>where now <span class="math notranslate nohighlight">\(0 &lt; \xi_n &lt; t / \sqrt{n}\)</span>.  Note that as <span class="math notranslate nohighlight">\(n\to \infty\)</span> the <span class="math notranslate nohighlight">\(\xi_n\)</span> gets pinched to 0 and therefor</p>
<div class="math notranslate nohighlight">
\[ m''_Z(\xi_n) t^2 /2 \to m''_Z(0) t^2 / 2 = E(Z^2) t^2/2 = t^2/2 \]</div>
<p>because the variance is 1 and the mean is 0:  <span class="math notranslate nohighlight">\( V(Z) = E(Z^2) - E(Z)^2 \)</span></p>
<p>Recall that if <span class="math notranslate nohighlight">\(\lim_{n\to \infty} b_n = b \)</span> then</p>
<div class="math notranslate nohighlight">
\[ \lim_{n\to \infty} ( 1 + \frac{b_n}{n} )^n = e^b \]</div>
<p>and we have that the moment generating functions converge to the moment generating function of the standard normal regardless of what the underlying distribution for the sample is!</p>
<div class="math notranslate nohighlight">
\[ \lim_{n\to \infty} m_{U_n}(t) = \lim_{n\to \infty} \left[ 1 + \frac{m''_Z(\xi_n) t^2/2}{n} \right]^n = e^{t^2/2} \]</div>
</div>
<div class="section" id="when-does-clt-not-apply">
<h4>When does CLT not apply?<a class="headerlink" href="#when-does-clt-not-apply" title="Permalink to this headline">¶</a></h4>
<p>With a theorem as powerful as CLT it is important to think about the cases when it does not apply. There are three related clues:  We need to have an mean and a variance so <span class="math notranslate nohighlight">\(E(Y)\)</span> and <span class="math notranslate nohighlight">\(E(Y^2)\)</span> need to exist. And not that the proof relied on there being a moment generating function so we need to know that the function</p>
<div class="math notranslate nohighlight">
\[ m_Y(t) = E( e^{tY}) \]</div>
<p>exists and has a second derivative for some open interval containing <span class="math notranslate nohighlight">\(t=0\)</span>. This condition will also give us the existence of the mean and variance.</p>
<p>So what random variables do not have moment generating functions?</p>
</div>
</div>
</div>
<div class="section" id="using-clt">
<h2>Using CLT<a class="headerlink" href="#using-clt" title="Permalink to this headline">¶</a></h2>
<p>So let’s now take a look at how we can use the Central Limit Theorem:</p>
<p>Suppose we are looking for how much the price of gas is changing in our city. Earlier in this course we sampled a few of the gas stations near us:  we have found through many observations that the variance of gas prices is around <span class="math notranslate nohighlight">\(0.32^2\)</span> dollars squared per gallons squared. We sample 10 stations nearby and find that the mean price <em>of the sample</em> is 2.98 dollars per gallon.</p>
<p>We do not know what the distribution of gas prices is, though we suspect it could be normal, but CLT tells us it does not matter if we are interested in the mean price of gas provided that the sample size is getting large - we do have a question of whether 10 is large enough.</p>
<p>Assuming that <span class="math notranslate nohighlight">\(\bar{Y}\)</span> is approximately normal, let’s find an interval in which we are 95% confident the population mean <span class="math notranslate nohighlight">\(\mu\)</span> is contained.</p>
<p>The variable</p>
<div class="math notranslate nohighlight">
\[ Z = \frac{\bar{Y} - \mu}{ \sigma/\sqrt{n}} = \frac{2.98 - \mu}{0.32 / \sqrt{10}} \]</div>
<p>is then approximately a standard normal random variable. We can find an interval centered at 0 in which we are 95% of the time we do this experiment we will find the value of <span class="math notranslate nohighlight">\(Z\)</span>.</p>
<p>If we want the interval <span class="math notranslate nohighlight">\((-a, a)\)</span> to have 0.95 of the porbability:</p>
<div class="math notranslate nohighlight">
\[ \int_{-a}^a \frac{1}{\sqrt{2\pi}} e^{-x^2/2} dx = 0.95 \]</div>
<p>then we want the tails of this integral to have 0.05 together or 0.025 each:</p>
<div class="math notranslate nohighlight">
\[ \int_{-\infty}^{-a} \frac{1}{\sqrt{2\pi}} e^{-x^2/2} dx = \int_{a}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-x^2/2} dx = 0.025 \]</div>
<p>In other words <span class="math notranslate nohighlight">\(-a\)</span> is the value for which the CDF gives 0.025. We can use the R command <em>qnorm</em> to take the inverse of the CDF and find <span class="math notranslate nohighlight">\(-a\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">qnorm</span><span class="p">(</span><span class="m">0.025</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">-1.95996398454005</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># It&#39;s a compute so lets use it to keep track of a</span>

<span class="n">a</span> <span class="o">=</span> <span class="o">-</span> <span class="nf">qnorm</span><span class="p">(</span><span class="m">0.025</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
<span class="n">a</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">1.95996398454005</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># I always check that I used the inverse correctly and check the tails</span>
<span class="m">1</span> <span class="o">-</span> <span class="nf">pnorm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.025</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">pnorm</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.025</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># we can also check the interior</span>

<span class="nf">pnorm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span> <span class="o">-</span> <span class="nf">pnorm</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">0.95</div></div>
</div>
<p>So we have:</p>
<div class="math notranslate nohighlight">
\[0.95 = P( -a &lt; Z &lt; a)= P( -a &lt; \frac{2.98 - \mu}{0.32 / \sqrt{10}} &lt; a) \]</div>
<p>and we can then solve this for an inequality bounding <span class="math notranslate nohighlight">\(\mu\)</span> in an interval:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="m">2.98</span> <span class="o">-</span> <span class="n">a</span><span class="o">*</span><span class="m">0.32</span> <span class="o">/</span> <span class="nf">sqrt</span><span class="p">(</span><span class="m">10</span><span class="p">);</span> <span class="m">2.98</span> <span class="o">+</span> <span class="n">a</span><span class="o">*</span><span class="m">0.32</span> <span class="o">/</span> <span class="nf">sqrt</span><span class="p">(</span><span class="m">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">2.78166558966254</div><div class="output text_html">3.17833441033746</div></div>
</div>
<div class="section" id="what-do-we-mean-by-95-confident">
<h3>What do we mean by 95% confident<a class="headerlink" href="#what-do-we-mean-by-95-confident" title="Permalink to this headline">¶</a></h3>
<p>Our statement is that we are 95% confident that <span class="math notranslate nohighlight">\(\mu\)</span> lies between these two numbers (a quick check is to verify that our sample mean is between these values). What we mean by this is that if we were to repeat the experiment:  sample 10 gas stations for their price with nothing else changing, we would 95% of the time (if we did it enough) find an interval that contains <span class="math notranslate nohighlight">\(\mu\)</span>. It is a sort of a statement about how likely it is that we have made the wrong conclusion.</p>
<p>Of course there are some assumptions we’ve made:</p>
<ul class="simple">
<li><p>We assumed that the sample was a random sample of independent identically distributed random variables. Was it?</p></li>
<li><p>We assumed that with a sample of size 10 the normal approximation to <span class="math notranslate nohighlight">\(\bar{Y}\)</span> would be accurate. Is it?</p></li>
<li><p>We assumed that variance we had computed over time was the variance of our distribution. Is it?</p></li>
</ul>
<p>This last point is a bit sticky. Frequently we will not know our variance and will have to approximate it from the sample itself. We will come back to this.</p>
</div>
<div class="section" id="some-questions">
<h3>Some questions<a class="headerlink" href="#some-questions" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>What happens if you decrease or increase our confidence level?</p></li>
<li><p>What happens if we decrease or increase our sample size?</p></li>
<li><p>How large of a sample would we need to know that we have the mean price to within 0.05 dollars with 95</p></li>
</ol>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            kernelName: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="06-Gamma_and_beta.html" title="previous page">Gamma and Beta Distributions</a>
    <a class='right-next' id="next-link" href="99-Apendices.html" title="next page">Class Schedule and Activities</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>