{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Intervals and Hypothesis Testing\n",
    "\n",
    "In the next two lessons we are going to expand upon the idea of Confidence Intervals that we saw as an example of how the CLT is used. Along the way we are going to explore more generally how point estimators are used, and deal with one of the sticky parts we saw in our last class - what can we do if we do not know the variance of the distribution we are sampling from?\n",
    "\n",
    "Finally we will then flip the construction around and show that we can use it to set up a process for making conclusions about the statistics of our distribution computed from the sample we have found.\n",
    "\n",
    "\n",
    "## Point Estimators\n",
    "\n",
    "An estimator is a rule for computing an a parameter for the underlying population from a sample that has been collected. In our class we explored the estimator $\\bar{Y}$ for the population mean $\\mu$ in great detail. $\\bar{Y}$ is an example of a *point estimator* and then the confidence itnerval we computed is an example of another type of estimator. Any parameter describing the distribution is suitable to find estimators for and we will give some common ones below. \n",
    "\n",
    "### Example\n",
    "\n",
    "As an example though. Let $Y$ be the uniform random variable on the interval $[0, a]$. That is $Y$ has PDF:\n",
    "\n",
    "$$ f(x) = \\left\\{ \\begin{matrix} x/a & x \\in [0, a] \\\\ 0 & \\mbox{otherwise} \\end{matrix} \\right. $$\n",
    "\n",
    "The problem being that we do not know what $a$ is. Suppose that we can sample from our distribution, so we collect a sample and find $1, 1.5, 2, 2, 2.3, 3.2$.\n",
    "\n",
    "What do we think $a$ is?  Can we find an estimate for it?\n",
    "\n",
    "### Biased Estimators \n",
    "\n",
    "Suppose that $\\hat{\\theta}$ is a point estimator for $\\theta$. We say that $\\hat{\\theta}$ is biased if $E(\\hat{\\theta}) \\neq \\theta$.  \n",
    "\n",
    "The Mean Square Error of an estimator is given by:\n",
    "$$ MSE(\\hat{\\theta}) = E( (\\hat{\\theta} - \\theta)^2 ) $$\n",
    "\n",
    "and there are two components to it. There is the underlying error caused by the random variation in the sampling procedure $V(\\hat{\\theta})$ and then there is error caused by the bias:\n",
    "\n",
    "$$ MSE(\\hat{\\theta}) = V(\\hat{\\theta}) + ( B(\\hat{\\theta}) )^2 $$\n",
    "\n",
    "#### Example $\\bar{Y}$\n",
    "\n",
    "$\\bar{Y}$ is an unbiased estimator for $\\mu$.\n",
    "\n",
    "#### Example $S$\n",
    "\n",
    "We are now in a position to explain why our sample variance is different from what we might expect:\n",
    "\n",
    "Let $Y_1, Y_2, \\dots, Y_n$ be a random sample with mean $\\mu$ and variance $\\sigma^2$:\n",
    "\n",
    "$$ S'^2 = \\frac{1}{n} \\sum (Y_i - \\bar{Y})^2 $$ \n",
    "\n",
    "is **NOT** the sample variance, however it is an estimator for the population variance. It is however biased:\n",
    "\n",
    "$$ E(S'^2) = \\frac{1}{n} E( \\sum (Y_i - \\bar{Y})^2) = \\frac1n E(\\sum Y_i^2) - E(\\bar{Y}^2) = \\sum  \\frac{1}{n} E(Y_i^2) - E(\\bar{Y}^2) $$\n",
    "\n",
    "We have that \n",
    "\n",
    "$$ E(Y_i^2) = V(Y_i) + E(Y_i)^2 = \\sigma^2 + \\mu^2 $$ and that \n",
    "\n",
    "$$ E(\\bar{Y}^2) = V(\\bar{Y}) + E( \\bar{Y})^2 = \\sigma^2 / n + \\mu^2 $$\n",
    "\n",
    "Putting it all together we have:\n",
    "\n",
    "$$ E(S'^2) = \\frac1n \\sum_{i=1}^n (\\sigma^2 + \\mu^2) - \\frac{\\sigma^2}{n} - \\mu^2 = \\sigma^2 - \\frac{1}{n} \\sigma^2  $$\n",
    "\n",
    "We can turn this into an unbiased estimator of $\\sigma^2$ by multiplying it by the factor $\\frac{n}{n-1}$ giving our definition of sample variance:\n",
    "\n",
    "$$ S^2 = \\frac{1}{n-1} \\sum (Y_i - \\bar{Y})^2 $$\n",
    "\n",
    "### Common Point Estimators\n",
    "\n",
    "Common unbiased point estimators we will use, together with their standard errors are:\n",
    "\n",
    "#### Single Normal Distribution\n",
    "\n",
    "To estimate $\\mu$ we will use $\\bar{Y}$. The standard error is $\\sigma/\\sqrt{n}$ where $\\sigma^2$ is the variance of the distribution. \n",
    "\n",
    "#### Single Bernouli Trial\n",
    "\n",
    "To estimate $p$ the probability of success in an individual trial we use $\\hat{p} = \\frac{Y}{n} $ the proportion of success in our sample of size n. The standard erorr is $\\sqrt{p (1-p){n} } $.\n",
    "\n",
    "#### Two Normal Distributions\n",
    "\n",
    "To estimate $\\mu_1 - \\mu_2$ the difference in the means of two populations, we use $\\bar{Y}_1 - \\bar{Y}_2$ the differences in the means of samples from the two population of size $n_1$ and $n_2$. The standard error is then: \n",
    "\n",
    "$$ \\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} } $$\n",
    "\n",
    "#### Two Bernouli Trials\n",
    "\n",
    "To estimate $p_1 - p_2$ the differences in the probability of success in individual trials we use $\\hat{p_1} - \\hat{p_2}$ the differences of the proportions of success in two samples of size $n_1$ and $n_2$. The standard error is then:\n",
    "\n",
    "$$ \\sqrt{ \\frac{p_1 (1-p_1)}{n_1} + \\frac{ p_2 (1-p_2) }{n_2 } } $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-Distributions\n",
    "\n",
    "#### Chi-squared Distributions\n",
    "\n",
    "#### F Distributions\n",
    "\n",
    "\n",
    "## Confidence Intervals for the Mean\n",
    "\n",
    "## Hypothesis Tests for the Mean\n",
    "\n",
    "## General Confidence Intervals and Hypothesis Tests\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
