{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from Distributions\n",
    "\n",
    "Let us start by recapping where we are:  We have met both generic and some particular *named* distributions that model situations, and in the case of *named* distributions that have sufficient structure or symmetry that we can compute with them.\n",
    "\n",
    "A couple of problems then confront us:  In reality we rarely know the distribution or we may know the *type* of distribution but not the exact parameters for a random variable, and our only way to gather information about the distribution is to sample from it. \n",
    "\n",
    "An analogy:  We are trying to identify a movie. Maybe we know it is a spy thriller. But otherwise all we have are some still images from it. How many images would we need to identify the movie? \n",
    "\n",
    "We will take class to sample from our *named* distributions and see what happens. I'll give one example here to illustrate the method:  Consider a shopping center where on average 2 customers arrive at the chashiers every minute. The number of customers $Y$ that will arrive at the chasiers in a minute is then a random variable given by the Poisson distribution:  \n",
    "\n",
    "$$ P(Y=x) = \\frac{2^r}{r!} e^{-2} $$\n",
    "\n",
    "Suppose we measure the number of customers arriving each minute for 5 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>0</li><li>3</li><li>2</li><li>4</li><li>0</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 0\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0\n",
       "2. 3\n",
       "3. 2\n",
       "4. 4\n",
       "5. 0\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0 3 2 4 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = rpois(5, 2)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some questions:\n",
    "\n",
    "- What is the mean number of customers in our sample?\n",
    "- What do we get as we increase the size of the sample?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1.925"
      ],
      "text/latex": [
       "1.925"
      ],
      "text/markdown": [
       "1.925"
      ],
      "text/plain": [
       "[1] 1.925"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = rpois(200, 2)\n",
    "mean(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we were to treat the mean of the sample $\\bar{Y}$ as a random variable itself. Fix the size of the sample and see what we get if we run the experiment many times, what shape do you see in the histogram for values of $\\bar{Y}$ and what happens to that shape as the size of the sample is then increased?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Run the experiment above with a variety of our named distributions including the normal distribution. What is happening?  Can you develop a hypothesis?\n",
    "\n",
    "## Multivariate Distributions\n",
    "\n",
    "In an effort to make our hypothesis we have develop precise, we need to build up some langauge for modeling this situation. \n",
    "\n",
    "A *multivariate distribution* is defined to be a distribution on multiple random variables describing the likliehood that values of them will come together. Explicitly given a set of discrete random variables $Y_1, Y_2, \\dots, Y_n$ we can define their joint probability distribution by:  $P(Y_1 = x_1, Y_2 = x_2, \\dots, Y_n = x_n) $ to be the probability that each of the $Y$s has the given value.\n",
    "\n",
    "In the continuous case, we need to proceed as we did before:  We define the joint cummulative distribution by:\n",
    "\n",
    "$$ F(x_1, x_2, \\dots, x_n) = P( Y_1 \\leq x_1, Y_2 \\leq x_2, \\dots, Y_n \\leq x_n) $$\n",
    "\n",
    "and then the joint probability density function is given by:\n",
    "\n",
    "$$ f(x_1, x_2, \\dots, x_n) = \\frac{\\partial^n F}{\\partial x_1 \\partial x_2 \\dots \\partial x_n$$\n",
    "\n",
    "Again do not fall for the trap of thinking about the density as the distribution:  the values of $f$ do not give the likliehood of a particular outcome.\n",
    "\n",
    "### Marginal Probability\n",
    "\n",
    "Given two discrete random variables $Y_1$ and $Y_2$: we define the marginal probability of $Y_1$ to be the total likliehood that $Y_1$ occurs.\n",
    "\n",
    "$$ P(Y_1 = x_1) = \\sum_{x_2} P(Y_1 = x_1, Y_2 = x_2) $$ \n",
    "\n",
    "In the continuous case the marginal PDF for $Y_1$ is found by:\n",
    "\n",
    "$$ f_1(x_1) = \\int f(x_1, x_2) dx_2 $$\n",
    "\n",
    "### Conditional Probability\n",
    "\n",
    "Consider two discrete random variables: We define the conditional probability $P(Y_1=x_1 | Y_2 = x_2)$, the probability that $Y_1 = x_1$ given that we have observed that $Y_2 = x_2$. This can be compute by recognizing that the likliehood of $Y_1$ occuring given a value for $Y_2$ is the likliehood that both values occured randomly divided by the marginal probablity of $Y_1$:\n",
    "\n",
    "$$ P(Y_1 = x_1 | Y_2 = x_2) = \\frac{P(Y_1 =x_1, Y_2 = x_2)}{P(Y_2 = x_2)}$$\n",
    "\n",
    "In other words the conditional probability given $Y_2=x_2$ is the proportion of the time that $Y_1=x_1$ occured out of all cases where $Y_2 = x_2$ occured.\n",
    "\n",
    "In the continuous case, the conditional PDF is found by:\n",
    "\n",
    "$$ f(x_1 | x_2) = \\frac{f(x_1, x_2)}{ f_2(x_2) } $$\n",
    "\n",
    "### Idependent Random Variables\n",
    "\n",
    "Consider the case of two discrete random varialbes with distribution $P(Y_1 = x_1, Y_2 = x_2)$. The two variables are independent if the likliehood of them both occuring is just the product of each one indvididually occuring (the marginal probabilities of each):  \n",
    "\n",
    "$$ P(Y_1 = x_1, Y_2 = x_2) = P(Y_1 = x_1) P(Y_2 = x_2) $$\n",
    "\n",
    "equivalently what we are saying is that the variables are independent if conditioning by one of them just gives the marginal likliehood:\n",
    "\n",
    "$$ P(Y_1 = x_1 | Y_2 = x_2) = P(Y_1 = x_1) $$\n",
    "\n",
    "In the continuous case, indepdent random variables have a joint PDF that is just a product of marginal PDFs:\n",
    "\n",
    "$$ f(x_1, x_2) = f_1(x_1) f_2(x_2) $$\n",
    "\n",
    "or in terms of the conditional PDF we will have:\n",
    "\n",
    "$$ f(x_1 | x_2) = f_1(x_1) $$\n",
    "\n",
    "Again the idea is that if the variables are independent, knowing the value we got for one of them should not change our probability density of the other one. \n",
    "\n",
    "#### Example\n",
    "\n",
    "Find the constant $c$ such that \n",
    "\n",
    "$$ f(x_1, x_2) = \\left\\{ \\begin{matrix} C x_1 x_2 & 0 \\leq x_1, x_2 \\leq 1 \\\\ 0 & \\mbox{otherwise} \\end{matrix} \\right. $$\n",
    "\n",
    "is a valid PDF. Are $x_1$ and $x_2$ indepdent?\n",
    "\n",
    "#### Example 2\n",
    "\n",
    "Find the constant $c$ such that \n",
    "\n",
    "$$ f(x_1, x_2) = \\left\\{ \\begin{matrix} C x_1 x_2 & 0 \\leq x_1 < x_2 \\leq 1 \\\\ 0 & \\mbox{otherwise} \\end{matrix} \\right. $$\n",
    "\n",
    "is a valid PDF. Are $x_1$ and $x_2$ indepdent?\n",
    "\n",
    "Reminder:  You can use wolfram alpha to compute the integral, but you may need to think about how to set it up correctly. You could determine independence/dependence without actually doing the integral if you appeal to the meaning of the definitions above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and Covariance\n",
    "\n",
    "Given a pair of jointly distributed continuous random variables $Y_1$ and $Y_2$ one measure of their dependence is to ask how far they jointly move from the means. Let \n",
    "\n",
    "$$ \\mu_1 = E(Y_1) \\qquad \\mbox{and} \\qquad \\mu_2 = E(Y_2) $$ \n",
    "\n",
    "we have the variances\n",
    "\n",
    "$$ \\sigma_1^2 = E( (Y_1 - \\mu_1)^2) \\qquad \\mbox{and} \\qquad \\sigma_2^2 = E( (Y_2 - \\mu_2)^2 ) $$ \n",
    "\n",
    "which again measure the extent to which $Y_1$ (resp. $Y_2$) is likely to be far from its mean.\n",
    "\n",
    "However, it is also interesting to ask how far $Y_1$ will stray from its mean while $Y_2$ is simmultaneously measured from its mean. We defin the covariance of $Y_1$ and $Y_2$ to be\n",
    "\n",
    "$$ \\mbox{Cov}(Y_1, Y_2) = E( (Y_1 - \\mu_1) (Y_2 - \\mu_2) ) $$\n",
    "\n",
    "Note that if $Y_1$ and $Y_2$ are independent then \n",
    "\n",
    "$$ \\mbox{Cov}(Y_1, Y_2) = E( (Y_1 -\\mu_1) ) E( (Y_2 - \\mu_2) ) = 0 \\cdot 0 = 0 $$\n",
    "\n",
    "The units of $\\mbox{Cov}(Y_1, Y_2)$ are the products of the two units. Which means that this is not an absolute measure of the correlations between two varaibles; for example if one of the variables has a large variance it will mean a larger covariance even if the correlation is weak. We can get something that is unitless by dividing this by the square root of the product of variances. The *coefficient of correlation* is iven by\n",
    "\n",
    "$$ \\rho = \\frac{\\mbox{Cov}(Y_1, Y_2)}{\\sqrt{ V(Y_1) V(Y_2) }}  $$\n",
    "\n",
    "### Algebra\n",
    "\n",
    "Some algebra of the expected values gives:\n",
    "\n",
    "$$ \\mbox{Cov}(Y_1, Y_2) = E( Y_1 Y_2) - E(Y_1) E(Y_2) $$\n",
    "\n",
    "and so one way to think about covariance is that it is measuring how far the expected value of the product varies from what it would be if the variables were not correlated.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Be careful no correlation does not imply indepdence. Consider the joint density:\n",
    "\n",
    "$$ f(x_1, x_2) = \\left\\{ \\begin{matrix} C & -1 < x_1 < 0; \\qquad 0 < x_2 < 1 + x_1 \\\\ C & 0 < x_1 < 1; \\qquad 0 < x_2 < 1-x_1 \\\\ 0 & \\mbox{otherwise} \\end{matrix} \\right. $$\n",
    "\n",
    "Show that $\\mbox{Cov}(Y_1, Y_2) = 0$ but that $Y_1$ and $Y_2$ are dependent. \n",
    "\n",
    "### Linear Combinations of Independent Random Variables\n",
    "\n",
    "Suppose that $Y_1, \\dots, Y_n$ are independent random variables with $E(Y_i) = \\mu_i$ and $V(Y_i) = \\sigma_i$. Let $U = \\sum a_i Y_i $ and $V = \\sum b_i Y_i$ for some coefficients $a_i$ and $b_i$. What can you say about \n",
    "\n",
    "- $ E(U)$ and $E(V)$?\n",
    "- $ V(U)$ and $V(V)$?\n",
    "- $\\mbox{Cov}(U, V)$?\n",
    "\n",
    "- when will $U$ and $V$ have no correlation?\n",
    "- when will $U$ and $V$ be independent?\n",
    "- when will $U$ and $V$ have no correlation but be dependent?\n",
    "\n",
    "## Random Sampling\n",
    "\n",
    "This then brings us back to our sampling question. We can think of a random sample from a distribution, $Y_1, Y_2, \\dots, Y_n$ as a multivariate distribution of independent identically distributed random variables. \n",
    "\n",
    "If the $Y_i$ are discrete with distribution $P(Y = x) = p(x)$ then:\n",
    "\n",
    "$$ P(Y_1 = x_1, Y_2 = x_2, \\dots, Y_n = x_n) = p(x_1) p(x_2) \\dots p(x_n) $$ \n",
    "\n",
    "If the $Y_i$ are continuous with PDF $f(x)$ then:\n",
    "\n",
    "$$ f(x_1, x_2, \\dots, x_n) = f(x_1) f(x_2) \\dots f(x_n)$$\n",
    "\n",
    "We can then answer questions like:  \n",
    "\n",
    "### Mean of the sample\n",
    "\n",
    "The mean of the sample is given by:\n",
    "\n",
    "$$ \\bar{Y} = \\frac{1}{n} Y_1 + \\frac{1}{n} Y_2 + \\dots + \\frac{1}{n} Y_n $$\n",
    "\n",
    "Suppose we are in the continuous case, and we want to know what the distribution of $\\bar{Y}$ is?\n",
    "\n",
    "Well its the same trick we have used over-and-over, we compute the CDF and then differentiate:\n",
    "\n",
    "$$ F(x) = P( \\bar{Y} < x ) = \\iint \\dots \\int_{x_1 + x_2 + \\dots + x_n \\leq n x} f(x_1) f(x_2) \\dots f(x_n) dx_1 dx_2 \\dots dx_n $$\n",
    "\n",
    "Okay so there is not much to do unless we have an explicit value for the PDF, and even then it is likely to get hairy quickly. However two points (a) it is exaclty a problem that a symbolic computer system is good at, and (b) it should not be surprising that we can compute with this. \n",
    "\n",
    "We will do one example:\n",
    "\n",
    "#### Sample from a Normal Distribution\n",
    "\n",
    "Let $Y_1$ and $Y_2$ be random samples from the Standard Normal distribution ( the normal distribution with mean 0 and variance 1). \n",
    "\n",
    "The PDF for each is:\n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2} $$\n",
    "\n",
    "If the mean of the sample is then $\\bar{Y} = \\frac12 Y_1 + \\frac12 Y_2 $ then the CDF for the mean is:\n",
    "\n",
    "$$ F(x) = \\iint_{x_1 + x_2 < 2 x} \\frac{1}{2\\pi} e^{-x_1^2 - x_2^2} dx_1 dx_2 $$\n",
    "\n",
    "We can set up this itereated integral by fixing an $x_1$ and then integrating over all $x_2$ values such that the inequality holds, and then integrating over all possible $x_1$ values:\n",
    "\n",
    "$$ F(x) = \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty dx_1 e^{-x_1^2} \\int_{-\\infty}^{2x - x_1} dx_2 e^{-x_2^2} $$\n",
    "\n",
    "Of course we don't actually care what this is, because what we want to do is differentiate it to get the PDF. Notice that the $x$ dependence is confined to just one bound on the iterated integral. We get:\n",
    "\n",
    "$$ f(x) = \\frac{dF}{dx} = \\frac{1}{\\pi} \\int_{-\\infty}^\\infty dx_1 e^{-x_1^2} e^{-(2x - x_1)^2} = \\frac{1}{\\pi} \\int_{-\\infty}^\\infty dx_1 e^{-4x^2 + 4x x_1 - 2x_1^2} $$\n",
    "\n",
    "If we complete the square in the exponent we get:\n",
    "\n",
    "$$ - 2( x_1^2 - 2 x x_1) - 4 x^2 = - 2 ( x_1 -x)^2 - 2 x^2 $$\n",
    "\n",
    "Then the integral is just the normalizing constant for a normal random variable with variance $\\sigma^2 = 1/2$ and mean $\\mu = x$ and so it gives $\\sqrt{\\pi}$. Therefore we have\n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sqrt{pi}} e^{- 2 x^2} $$ \n",
    "\n",
    "Which is the PDF for a normal distribution with mean 0 and a variance of $1/2$. \n",
    "\n",
    "### Minimum of the sample\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
